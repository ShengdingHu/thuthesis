
@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{voronov2023loss,
  title={Is This Loss Informative? Speeding Up Textual Inversion with Deterministic Objective Evaluation},
  author={Voronov, Anton and Khoroshikh, Mikhail and Babenko, Artem and Ryabinin, Max},
  journal={arXiv preprint arXiv:2302.04841},
  year={2023}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}


@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12104--12113},
  year={2022}
}


@article{schaeffer2023emergent,
  title={Are emergent abilities of Large Language Models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2304.15004},
  year={2023}
}


@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{li2023starcoder,
  title={StarCoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@inproceedings{liu2023same,
  title={Same pre-training loss, better downstream: Implicit bias matters for language models},
  author={Liu, Hong and Xie, Sang Michael and Li, Zhiyuan and Ma, Tengyu},
  booktitle={International Conference on Machine Learning},
  pages={22188--22214},
  year={2023},
  organization={PMLR}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{dubois2023alpacafarm,
  title={Alpacafarm: A simulation framework for methods that learn from human feedback},
  author={Dubois, Yann and Li, Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2305.14387},
  year={2023}
}


@article{qian2023communicative,
  title={Communicative agents for software development},
  author={Qian, Chen and Cong, Xin and Yang, Cheng and Chen, Weize and Su, Yusheng and Xu, Juyuan and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2307.07924},
  year={2023}
}

@article{varma2023explaining,
  title={Explaining grokking through circuit efficiency},
  author={Varma, Vikrant and Shah, Rohin and Kenton, Zachary and Kram{\'a}r, J{\'a}nos and Kumar, Ramana},
  journal={arXiv preprint arXiv:2309.02390},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{muennighoff2023scaling,
  title={Scaling Data-Constrained Language Models},
  author={Muennighoff, Niklas and Rush, Alexander M and Barak, Boaz and Scao, Teven Le and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
  journal={arXiv preprint arXiv:2305.16264},
  year={2023}
}

@article{lu2023emergent,
  title={Are Emergent Abilities in Large Language Models just In-Context Learning?},
  author={Lu, Sheng and Bigoulaeva, Irina and Sachdeva, Rachneet and Madabushi, Harish Tayyar and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2309.01809},
  year={2023}
}

@article{nolfi2023unexpected,
  title={On the Unexpected Abilities of Large Language Models},
  author={Nolfi, Stefano},
  journal={arXiv preprint arXiv:2308.09720},
  year={2023}
}

@article{hagendorff2023machine,
  title={Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods},
  author={Hagendorff, Thilo},
  journal={arXiv preprint arXiv:2303.13988},
  year={2023}
}

@article{sorscher2022beyond,
  title={Beyond neural scaling laws: beating power law scaling via data pruning},
  author={Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19523--19536},
  year={2022}
}

@inproceedings{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}

@article{bahri2021explaining,
  title={Explaining neural scaling laws},
  author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
  journal={arXiv preprint arXiv:2102.06701},
  year={2021}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{yao2023research,
  title={Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales},
  author={Yao, Yiqun and Wang, Yequan},
  journal={arXiv preprint arXiv:2304.06875},
  year={2023}
}

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@article{DBLP:journals/corr/abs-2002-05202,
  author       = {Noam Shazeer},
  title        = {{GLU} Variants Improve Transformer},
  journal      = {CoRR},
  volume       = {abs/2002.05202},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.05202},
  eprinttype    = {arXiv},
  eprint       = {2002.05202},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05202.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{wang2022deepnet,
  title={Deepnet: Scaling transformers to 1,000 layers},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  journal={arXiv preprint arXiv:2203.00555},
  year={2022}
}

@inproceedings{ganguli2022predictability,
  title={Predictability and surprise in large generative models},
  author={Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Dassarma, Nova and Drain, Dawn and Elhage, Nelson and others},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1747--1764},
  year={2022}
}

@article{elhage2021mathematical,
title={A mathematical framework for {T}ransformer circuits.},
booktitle={
    Transformer Circuits Thread
},
year={2021},
url={https://transformer-circuits.pub/2021/framework/index.html},
author={Nelson, Elhage and Neel,  Nanda and Catherine, Olsson and  Tom, Henighan and  Nicholas, Joseph and  Ben, Mann and  Amanda, Askell and  Yuntao, Bai and  Anna, Chen and  Tom, Conerly and  Nova, DasSarma and  Dawn, Drain and  Deep, Ganguli and  Zac, Hatfield-Dodds and  Danny, Hernandez and  Andy, Jones and  Jackson, Kernion and  Liane, Lovitt and  Kamal, Ndousse and  Dario, Amodei and  Tom, Brown and  Jack, Clark and  Jared, Kaplan and  Sam, McCandlish and  Chris, Olah}
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}


@InProceedings{pmlr-v9-glorot10a,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}
 
@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@inproceedings{Vaswani+2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{yang2022tensor,
  title={Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022}
}

@article{gupta2023continual,
  title={Continual Pre-Training of Large Language Models: How to (re) warm your model?},
  author={Gupta, Kshitij and Th{\'e}rien, Benjamin and Ibrahim, Adam and Richter, Mats L and Anthony, Quentin and Belilovsky, Eugene and Rish, Irina and Lesort, Timoth{\'e}e},
  journal={arXiv preprint arXiv:2308.04014},
  year={2023}
}

@article{dey2023cerebras,
  title={Cerebras-GPT: Open compute-optimal language models trained on the Cerebras wafer-scale cluster},
  author={Dey, Nolan and Gosal, Gurpreet and Khachane, Hemant and Marshall, William and Pathria, Ribhu and Tom, Marvin and Hestness, Joel and others},
  journal={arXiv preprint arXiv:2304.03208},
  year={2023}
}

@article{yang2023tensor,
  title={Tensor programs vi: Feature learning in infinite-depth neural networks},
  author={Yang, Greg and Yu, Dingli and Zhu, Chen and Hayou, Soufiane},
  journal={arXiv preprint arXiv:2310.02244},
  year={2023}
}



@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{sardana2023beyond,
  title={Beyond chinchilla-optimal: Accounting for inference in language model scaling laws},
  author={Sardana, Nikhil and Frankle, Jonathan},
  journal={arXiv preprint arXiv:2401.00448},
  year={2023}
}


@article{DBLP:journals/corr/abs-2210-17323,
  author       = {Elias Frantar and
                  Saleh Ashkboos and
                  Torsten Hoefler and
                  Dan Alistarh},
  title        = {{GPTQ:} Accurate Post-Training Quantization for Generative Pre-trained
                  Transformers},
  journal      = {CoRR},
  volume       = {abs/2210.17323},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2210.17323},
  doi          = {10.48550/ARXIV.2210.17323},
  eprinttype    = {arXiv},
  eprint       = {2210.17323},
  timestamp    = {Thu, 03 Nov 2022 09:50:24 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2210-17323.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% quantization datasets

% shareGPT
@misc{ShareGPT, title={ShareGPT}, url={https://huggingface.co/datasets/openchat/openchat_sharegpt_v3/tree/main}, author={openchat} }

% eval-instruct
@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

% SlimOrca
@misc{SlimOrcaDedup,
  title = {SlimOrca Dedup: A Deduplicated Subset of SlimOrca},
  author = {Wing Lian and Guan Wang and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and "Teknium" and Nathan Hoos},
  year = {2023},
  publisher = {HuggingFace},
  url = {https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup/}
}

@misc{mukherjee2023orca,
      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, 
      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},
      year={2023},
      eprint={2306.02707},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{longpre2023flan,
      title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}, 
      author={Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},
      year={2023},
      eprint={2301.13688},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

% Glaive
@misc{Glaive, title={Glaive-Code-Assistant}, url={https://huggingface.co/datasets/glaiveai/glaive-code-assistant}, author={GlaiveAI} }

% MagiCoder
@misc{MagiCoder-evol-instruct, title={MagiCoder-Evol-Instruct}, url={https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K}, author={MagiCoder} }
@misc{MagiCoder-OSS-instruct, title={MagiCoder-OSS-Instruct}, url={https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K}, author={MagiCoder} }

% metamath
@article{yu2023metamath,
  title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

% mammoth
@article{yue2023mammoth,
  title={MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning},
  author={Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
  journal={arXiv preprint arXiv:2309.05653},
  year={2023}
}

% ultrachat
@misc{ding2023enhancing,
      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, 
      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},
      year={2023},
      eprint={2305.14233},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{tunstall2023zephyr,
      title={Zephyr: Direct Distillation of LM Alignment}, 
      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
      year={2023},
      eprint={2310.16944},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% ultrafeedback
@misc{cui2023ultrafeedback,
      title={UltraFeedback: Boosting Language Models with High-quality Feedback}, 
      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2310.01377},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{2019t5,
    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    journal = {arXiv e-prints},
    year = {2019},
    archivePrefix = {arXiv},
    eprint = {1910.10683},
}
% ultrainteract


@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{almazrouei2023falcon,
  title={The falcon series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023}
}


@article{liu2024mobilellm,
  title={MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases},
  author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others},
  journal={arXiv preprint arXiv:2402.14905},
  year={2024}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Gemini, Team and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}


@misc{li2024cmmlu,
      title={CMMLU: Measuring massive multitask language understanding in Chinese}, 
      author={Haonan Li and Yixuan Zhang and Fajri Koto and Yifei Yang and Hai Zhao and Yeyun Gong and Nan Duan and Timothy Baldwin},
      year={2024},
      eprint={2306.09212},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{huang2024c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@misc{austin2021program,
      title={Program Synthesis with Large Language Models}, 
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@misc{suzgun2022challenging,
      title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them}, 
      author={Mirac Suzgun and Nathan Scales and Nathanael Schärli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and Ed H. Chi and Denny Zhou and Jason Wei},
      year={2022},
      eprint={2210.09261},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={BIG-bench authors},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj},
  note={}
}

@article{gunasekar2023textbooks,
  title={Textbooks are all you need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}

@article{li2023textbooks,
  title={Textbooks are all you need ii: phi-1.5 technical report},
  author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}

@article{zhang2024tinyllama,
  title={Tinyllama: An open-source small language model},
  author={Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
  journal={arXiv preprint arXiv:2401.02385},
  year={2024}
}



@misc{Banks2024Gemma,
  author = {Banks, Jeanine and Warkentin, Tris},
  title = {Gemma: Introducing new state-of-the-art open models},
  year = {2024},
  howpublished = {\url{https://blog.google/technology/developers/gemma-open-models/}},
  note = {Accessed: date-of-access}
}


@misc{Javaheripi2023Phi2,
  author = {Javaheripi, Mojan and Bubeck, Sébastien},
  title = {Phi-2: The surprising power of small language models},
  year = {2023},
  howpublished = {\url{https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/}},
  note = {Accessed: date-of-access}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@article{wortsman2023small,
  title={Small-scale proxies for large-scale transformer training instabilities},
  author={Wortsman, Mitchell and Liu, Peter J and Xiao, Lechao and Everett, Katie and Alemi, Alex and Adlam, Ben and Co-Reyes, John D and Gur, Izzeddin and Kumar, Abhishek and Novak, Roman and others},
  journal={arXiv preprint arXiv:2309.14322},
  year={2023}
}


@article{xie2024doremi,
  title={Doremi: Optimizing data mixtures speeds up language model pretraining},
  author={Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy S and Le, Quoc V and Ma, Tengyu and Yu, Adams Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{shi2023context,
  title={In-Context Pretraining: Language Modeling Beyond Document Boundaries},
  author={Shi, Weijia and Min, Sewon and Lomeli, Maria and Zhou, Chunting and Li, Margaret and Lin, Victoria and Smith, Noah A and Zettlemoyer, Luke and Yih, Scott and Lewis, Mike},
  journal={arXiv preprint arXiv:2310.10638},
  year={2023}
}

@article{ye2024data,
  title={Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance},
  author={Ye, Jiasheng and Liu, Peiju and Sun, Tianxiang and Zhou, Yunhua and Zhan, Jun and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2403.16952},
  year={2024}
}

@article{smith2017don,
  title={Don't decay the learning rate, increase the batch size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017}
}

@article{bi2024deepseek,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}

@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}


@article{howard2018universal,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}


@article{hundt2019sharpdarts,
  title={sharpdarts: Faster and more accurate differentiable architecture search},
  author={Hundt, Andrew and Jain, Varun and Hager, Gregory D},
  journal={arXiv preprint arXiv:1903.09900},
  year={2019}
}


@article{hu2023unlock,
  title={Unlock predictable scaling from emergent abilities},
  author={Hu, Shengding and Liu, Xin and Han, Xu and Zhang, Xinrong and He, Chaoqun and Zhao, Weilin and Lin, Yankai and Ding, Ning and Ou, Zebin and Zeng, Guoyang and others},
  journal={arXiv preprint arXiv:2310.03262},
  year={2023}
}

@article{du2024understanding,
  title={Understanding Emergent Abilities of Language Models from the Loss Perspective},
  author={Du, Zhengxiao and Zeng, Aohan and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2403.15796},
  year={2024}
}

@inproceedings{aghajanyan2023scaling,
  title={Scaling laws for generative mixed-modal language models},
  author={Aghajanyan, Armen and Yu, Lili and Conneau, Alexis and Hsu, Wei-Ning and Hambardzumyan, Karen and Zhang, Susan and Roller, Stephen and Goyal, Naman and Levy, Omer and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={265--279},
  year={2023},
  organization={PMLR}
}

@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}


@article{du2021glm,
  title={Glm: General language model pretraining with autoregressive blank infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10360},
  year={2021}
}

@article{peng2023yarn,
  title={Yarn: Efficient context window extension of large language models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  journal={arXiv preprint arXiv:2309.00071},
  year={2023}
}

@article{wang2023cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv preprint arXiv:2311.03079},
  year={2023}
}

@article{bai2023qwenvl,
  title={Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{dolma,
  title = {{Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}},
  author={Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Nathan Lambert and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and Noah A. Smith and Hannaneh Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo},
  year={2024},
  journal={arXiv preprint},
  url={https://arxiv.org/abs/2402.00159}
}




@inproceedings{broder1997resemblance,
  title={On the resemblance and containment of documents},
  author={Broder, Andrei Z},
  booktitle={Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)},
  pages={21--29},
  year={1997},
  organization={IEEE}
}


@article{biderman2022datasheet,
  title={Datasheet for the pile},
  author={Biderman, Stella and Bicheno, Kieran and Gao, Leo},
  journal={arXiv preprint arXiv:2201.07311},
  year={2022}
}

@article{Kocetkov2022TheStack,
  title={The Stack: 3 TB of permissively licensed source code},
  author={Kocetkov, Denis and Li, Raymond and Ben Allal, Loubna and Li, Jia and Mou,Chenghao and Muñoz Ferrandis, Carlos and Jernite, Yacine and Mitchell, Margaret and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and von Werra, Leandro and de Vries, Harm},
  journal={Preprint},
  year={2022}
}


@misc{SlimOrca,
  title = {SlimOrca: An Open Dataset of GPT-4 Augmented FLAN Reasoning Traces, with Verification},
  author = {Wing Lian and Guan Wang and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and "Teknium"},
  year = {2023},
  publisher = {HuggingFace},
  url = {https://https://huggingface.co/Open-Orca/SlimOrca}
}

@article{wei2023magicoder,
  title={Magicoder: Source Code Is All You Need},
  author={Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  journal={arXiv preprint arXiv:2312.02120},
  year={2023}
}


@article{komatsuzaki2022sparse,
  title={Sparse upcycling: Training mixture-of-experts from dense checkpoints},
  author={Komatsuzaki, Aran and Puigcerver, Joan and Lee-Thorp, James and Ruiz, Carlos Riquelme and Mustafa, Basil and Ainslie, Joshua and Tay, Yi and Dehghani, Mostafa and Houlsby, Neil},
  journal={arXiv preprint arXiv:2212.05055},
  year={2022}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@article{young2024yi,
  title={Yi: Open foundation models by 01. ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}


@article{song2023zebra,
  title={Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention},
  author={Song, Kaiqiang and Wang, Xiaoyang and Cho, Sangwoo and Pan, Xiaoman and Yu, Dong},
  journal={arXiv preprint arXiv:2312.08618},
  year={2023}
}




@article{zhang2024infty,
  title={$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens},
  author={Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo Khai and Han, Xu and Thai, Zhen Leng and Wang, Shuo and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2402.13718},
  year={2024}
}

@article{xiong2023effective,
  title={Effective long-context scaling of foundation models},
  author={Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others},
  journal={arXiv preprint arXiv:2309.16039},
  year={2023}
}

@misc{bloc97_2023_ntk,
  author = {bloc97},
  title = {{NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation}},
  year = {2023},
  howpublished = {\url{https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}},
  note = {Accessed: [Insert Date of Access]}
}

@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@software{mlc-llm,
    author = {MLC team},
    title = {{MLC-LLM}},
    url = {https://github.com/mlc-ai/mlc-llm},
    year = {2023}
}

@software{llmfarm,
    author = {LLMFarm team},
    title = {{LLMFarm}},
    url = {https://github.com/guinmoon/LLMFarm},
    year = {2023}
}

@inproceedings{ainslie-etal-2023-gqa,
    title = "{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
    author = "Ainslie, Joshua  and
      Lee-Thorp, James  and
      de Jong, Michiel  and
      Zemlyanskiy, Yury  and
      Lebron, Federico  and
      Sanghai, Sumit",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.298",
    doi = "10.18653/v1/2023.emnlp-main.298",
    pages = "4895--4901",
}



@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}


@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}



@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{zhangkun1994,
  author     = {张昆 and 冯立群 and 余昌钰 and others},
  title      = {机器人柔性手腕的球面齿轮设计研究},
  journal    = {清华大学学报: 自然科学版},
  volume     = {34},
  number     = {2},
  pages      = {1--7},
  year       = {1994},
  key        = {zhang1 kun1},
}

@book{zhukezhen1973,
  author     = {竺可桢},
  title      = {物理学论},
  address    = {北京},
  publisher  = {科学出版社},
  year       = {1973},
  pages      = {56--60},
  key        = {zhu2 ke3 zhen1},
}

@inproceedings{dupont1974bone,
  author     = {Dupont, B},
  title      = {Bone marrow transplantation in severe combined immunodeficiency with an unrelated MLC compatible donor},
  editor     = {White, H J and Smith, R},
  booktitle  = {Proceedings of the third annual meeting of the International Society for Experimental Hematology},
  address    = {Houston},
  publisher  = {International Society for Experimental Hematology},
  year       = {1974},
  pages      = {44--46},
}

@mastersthesis{zhengkaiqing1987,
  author     = {郑开青},
  title      = {通讯系统模拟及软件},
  address    = {北京},
  school     = {清华大学无线电系},
  year       = {1987},
  key        = {zheng4 kai1 qing1},
}

@patent{jiangxizhou1980,
  author     = {姜锡洲},
  title      = {一种温热外敷药制备方案: 中国, 88105607.3},
  date       = {1980-07-26},
  key        = {jiang1 xi1 zhou1},
}

@standard{jianduju1994,
  author     = {中华人民共和国国家技术监督局},
  title      = {GB3100-3102. 中华人民共和国国家标准-量与单位},
  address    = {北京},
  publisher  = {中国标准出版社},
  year       = {1994},
  key        = {zhong1 hua2 ren2 min2 gong4 he2 guo2},
}

@article{merkt1995rotational,
  author     = {Merkt, Fr{\'e}d{\'e}ric and Mackenzie, S R and Softley, Timothy P},
  title      = {Rotational Autoionization Dynamics in High Rydberg States of Nitrogen},
  journal    = {J Chem Phys},
  year       = {1995},
  volume     = {103},
  pages      = {4509--4518},
}

@article{mellinger1996laser,
  author     = {Mellinger, A and Vidal, C R and Jungen, {Ch}},
  title      = {Laser reduced fluorescence study of the carbon monoxide nd triplet Rydberg series - Experimental results and multichannel quantum defect analysis},
  journal    = {J Chem Phys},
  year       = {1996},
  volume     = {104},
  pages      = {8913--8921},
}

@article{bixon1996dynamics,
  author     = {Bixon, M and Jortner, Joshua},
  title      = {The dynamics of predissociating high {Rydberg} states of {NO}},
  journal    = {J Chem Phys},
  year       = {1996},
  volume     = {105},
  pages      = {1363--1382},
}

@article{mahui1995,
  author     = {马辉 and 李俭 and 刘耀明 and others},
  title      = {利用 {REMPI} 方法测量 {BaF} 高里德堡系列光谱},
  journal    = {化学物理学报},
  year       = {1995},
  volume     = {8},
  pages      = {308--311},
  key        = {ma3 hui1},
}

@article{carlson1981two,
  author     = {Carlson, N W and Taylor, A J and Jones, K M and Schawlow, A L},
  title      = {Two-step polarization-labeling spectroscopy of excited states of {Na2}},
  journal    = {Phys Rev A},
  year       = {1981},
  volume     = {24},
  pages      = {822--834},
}

@article{taylor1983scanning,
  author     = {Taylor, A J and Jones, K M and Schawlow, A L},
  title      = {Scanning pulsed-polarization spectrometer applied to {Na2}},
  journal    = {J Opt Soc Am},
  year       = {1983},
  volume     = {73},
  pages      = {994--998},
}

@article{taylor1981study,
  author     = {Taylor, A J and Jones, K M and Schawlow, A L},
  title      = {A study of the excited {1$\Sigma$g+} states in {Na2}},
  journal    = {Opt Commun},
  year       = {1981},
  volume     = {39},
  pages      = {47--50},
}

@article{shimizu1983laser,
  author     = {Shimizu, Kazuko and Shimizu, Fujio},
  title      = {Laser induced fluorescence spectra of the a {3$\Pi$u--X 1$\Sigma$g+} band of {Na2} by molecular beam},
  journal    = {J Chem Phys},
  year       = {1983},
  volume     = {78},
  pages      = {1126--1131},
}

@article{atkinson1982experimental,
  author     = {Atkinson, J B and Becker, J and Demtr{\"o}der, W},
  title      = {Experimental observation of the a {3$\Pi$u} state of {Na2}},
  journal    = {Chem Phys Lett},
  year       = {1982},
  volume     = {87},
  pages      = {92--97},
}

@article{kusch1975perturbations,
  author     = {Kusch, P and Hessel, M M},
  title      = {Perturbations in the A {1$\Sigma$u+} state of {Na2}},
  journal    = {J Chem Phys},
  year       = {1975},
  volume     = {63},
  pages      = {4087--4088},
}

@book{guangxi1993,
  author     = {广西壮族自治区林业厅},
  title      = {广西自然保护区},
  address    = {北京},
  publisher  = {中国林业出版社},
  year       = {1993},
  key        = {guang3 xi1 zhuang4 zu2 zi4 zhi4 qu1},
}

@book{huosini1989guwu,
  author     = {霍斯尼},
  title      = {谷物科学与工艺学原理},
  translator = {李庆龙},
  edition    = {2},
  address    = {北京},
  publisher  = {中国食品出版社},
  year       = {1989},
  pages      = {15--20},
  key        = {huo4 si1 ni2},
}

@book{wangfuzhi1865songlun,
  author     = {王夫之},
  title      = {宋论},
  edition    = {刻本},
  address    = {金陵},
  publisher  = {曾氏},
  year       = {1865（清同治四年）},
  key        = {wang2 fu1 zhi1},
}

@book{zhaoyaodong1998xinshidai,
  author     = {赵耀东},
  title      = {新时代的工业工程师},
  address    = {台北},
  publisher  = {天下文化出版社},
  year       = {1998},
  urldate    = {1998-09-26},
  url        = {http://www.ie.nthu.edu.tw/info/ie.newie.htm},
  key        = {zhao4 yao4 dong1},
}

@standard{biaozhunhua2002tushu,
  author     = {全国信息与文献工作标准化技术委员会出版物格式分委员会},
  title      = {GB/T 12450-2001 图书书名页},
  address    = {北京},
  publisher  = {中国标准出版社},
  year       = {2002},
  pages      = {1},
  key        = {quan2 guo2 xin4 xi1},
}

@book{chubanzhuanye2004,
  author     = {全国出版专业职业资格考试办公室},
  title      = {全国出版专业职业资格考试辅导教材: 出版专业理论与实务•中级},
  edition    = {2014},
  address    = {上海},
  publisher  = {上海辞书出版社},
  year       = {2004},
  pages      = {299--307},
  key        = {quan2 guo2 chu1 ban3 ye4},
}

@techreport{who1970factors,
  author     = {{World Health Organization}},
  title      = {Factors Regulating the Immune Response: Report of {WHO Scientific Group}},
  address    = {Geneva},
  publisher  = {WHO},
  year       = {1970},
}

@book{peebles2001probability,
  author     = {Peebles, Jr, Peyton Z.},
  title      = {Probability, Random Variables, and Random Signal Principles},
  edition    = {4},
  address    = {New York},
  publisher  = {McGraw Hill},
  year       = {2001},
}

@incollection{baishunong1998zhiwu,
  author     = {白书农},
  title      = {植物开花研究},
  editor     = {李承森},
  booktitle  = {植物科学进展},
  address    = {北京},
  publisher  = {高等教育出版社},
  year       = {1998},
  pages      = {146--163},
  key        = {bai2 shu1 nong2},
}

@incollection{weinstein1974pathogenic,
  author     = {Weinstein, L and Swertz, M N},
  title      = {Pathogenic Properties of Invading Microorganism},
  editor     = {Sodeman, Jr, William A and Sodeman, William A},
  booktitle  = {Pathologic physiology: mechanisms of disease},
  address    = {Philadelphia},
  publisher  = {Saunders},
  year       = {1974},
  pages      = {745--772},
}

@inproceedings{hanjiren1985lun,
  author     = {韩吉人},
  title      = {论职工教育的特点},
  editor     = {中国职工教育研究会},
  booktitle  = {职工教育研究论文集},
  address    = {北京},
  publisher  = {人民教育出版社},
  year       = {1985},
  pages      = {90--99},
  key        = {han2 ji2 ren2},
}

@periodical{dizhi1936dizhi,
  author     = {中国地质学会},
  title      = {地质评论},
  year       = {1936},
  volume     = {1},
  number     = {1},
  address    = {北京},
  publisher  = {地质出版社},
  key        = {zhong1 guo2 di4 zhi3 xue2 hui4},
}

@periodical{tushuguan1957tushuguanxue,
  author     = {中国图书馆学会},
  title      = {图书馆学通讯},
  year       = {1957/1990},
  number     = {1--4},
  address    = {北京},
  publisher  = {北京图书馆},
  key        = {zhong1 guo2 tu2 shu1 guan3 xue2 hui4},
}

@periodical{aaas1883science,
  author     = {{American Association for the Advancement of Science}},
  title      = {Science},
  year       = {1883},
  volume     = {1},
  number     = {1},
  address    = {Washington, D.C.},
  publisher  = {American Association for the Advancement of Science},
}

@newspaper{fugang2000fengsha,
  author     = {傅刚 and 赵承 and 李佳路},
  title      = {大风沙过后的思考},
  journal    = {北京青年报},
  date       = {2000-04-12},
  number     = {14},
  urldate    = {2002-03-06},
  url        = {http://www.bjyouth.com.cn/Bqb/20000412/B/4216%5ED0412B1401.htm},
  key        = {fu4 gang1},
}

@online{xiaoyu2001chubanye,
  author     = {萧钰},
  title      = {出版业信息化迈入快车道},
  year       = {2001},
  date       = {2001-12-19},
  urldate    = {2002-04-15},
  url        = {http://www.creader.com/news/20011219/200112190019.htm},
  key        = {xiao1 yu4},
}

@online{oclc2000about,
  author     = {{Online Computer Library Center, Inc}},
  title      = {About {OCLC}: History of Cooperation},
  urldate    = {2000-01-08},
  url        = {http://www.oclc.org/about/cooperation.en.htm},
}

@software{scitor2000project,
  author     = {{Scitor Corporation}},
  title      = {Project scheduler},
  address    = {Sunnyvale, Calif.},
  publisher  = {Scitor Corporation},
  year       = {1983},
  medium     = {DK},
}

@article{ding2021openprompt,
 author = {Ding, Ning and Hu, Shengding and Zhao, Weilin and Chen, Yulin and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong},
 journal = {ArXiv preprint},
 title = {OpenPrompt: An Open-source Framework for Prompt-learning},
 url = {https://arxiv.org/abs/2111.01998},
 volume = {abs/2111.01998},
 year = {2021}
}



@article{jones1972statistical,
 author = {Jones, Karen Sparck},
 journal = {Journal of documentation},
 publisher = {MCB UP Ltd},
 title = {A statistical interpretation of term specificity and its application in retrieval},
 year = {1972}
}

@inproceedings{pedersen2004wordnet,
 address = {Boston, Massachusetts, USA},
 author = {Pedersen, Ted  and
Patwardhan, Siddharth  and
Michelizzi, Jason},
 booktitle = {Demonstration Papers at {HLT}-{NAACL} 2004},
 pages = {38--41},
 publisher = {Association for Computational Linguistics},
 title = {{W}ord{N}et::{S}imilarity - Measuring the Relatedness of Concepts},
 url = {https://aclanthology.org/N04-3012},
 year = {2004}
}

@inproceedings{speer2017conceptnet,
 author = {Robyn Speer and
Joshua Chin and
Catherine Havasi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/SpeerCH17.bib},
 booktitle = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence,
February 4-9, 2017, San Francisco, California, {USA}},
 editor = {Satinder P. Singh and
Shaul Markovitch},
 pages = {4444--4451},
 publisher = {{AAAI} Press},
 timestamp = {Fri, 31 May 2019 01:00:00 +0200},
 title = {ConceptNet 5.5: An Open Multilingual Graph of General Knowledge},
 url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972},
 year = {2017}
}

@inproceedings{nguyen2016ms,
 author = {Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
 booktitle = {Proceedings of CoCo@ NeurIPS},
 title = {MS MARCO: A human generated machine reading comprehension dataset},
 year = {2016}
}

@inproceedings{williams2018broad,
 address = {New Orleans, Louisiana},
 author = {Williams, Adina  and
Nangia, Nikita  and
Bowman, Samuel},
 booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
 doi = {10.18653/v1/N18-1101},
 pages = {1112--1122},
 publisher = {Association for Computational Linguistics},
 title = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
 url = {https://aclanthology.org/N18-1101},
 year = {2018}
}

@inproceedings{roberts2020much,
 address = {Online},
 author = {Roberts, Adam  and
Raffel, Colin  and
Shazeer, Noam},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.437},
 pages = {5418--5426},
 publisher = {Association for Computational Linguistics},
 title = {How Much Knowledge Can You Pack Into the Parameters of a Language Model?},
 url = {https://aclanthology.org/2020.emnlp-main.437},
 year = {2020}
}

@inproceedings{davison-etal-2019-commonsense,
 address = {Hong Kong, China},
 author = {Davison, Joe  and
Feldman, Joshua  and
Rush, Alexander},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1109},
 pages = {1173--1178},
 publisher = {Association for Computational Linguistics},
 title = {Commonsense Knowledge Mining from Pretrained Models},
 url = {https://aclanthology.org/D19-1109},
 year = {2019}
}

@inproceedings{zhang2015character,
 author = {Xiang Zhang and
Junbo Jake Zhao and
Yann LeCun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/ZhangZL15.bib},
 booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference
on Neural Information Processing Systems 2015, December 7-12, 2015,
Montreal, Quebec, Canada},
 editor = {Corinna Cortes and
Neil D. Lawrence and
Daniel D. Lee and
Masashi Sugiyama and
Roman Garnett},
 pages = {649--657},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Character-level Convolutional Networks for Text Classification},
 url = {https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html},
 year = {2015}
}

@article{lehmann2015dbpedia,
 author = {Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mendes, Pablo N and Hellmann, Sebastian and Morsey, Mohamed and Van Kleef, Patrick and Auer, S{\"o}ren and others},
 journal = {Semantic web},
 number = {2},
 pages = {167--195},
 publisher = {IOS Press},
 title = {Dbpedia--a large-scale, multilingual knowledge base extracted from wikipedia},
 volume = {6},
 year = {2015}
}

@inproceedings{maas2011learning,
 address = {Portland, Oregon, USA},
 author = {Maas, Andrew L.  and
Daly, Raymond E.  and
Pham, Peter T.  and
Huang, Dan  and
Ng, Andrew Y.  and
Potts, Christopher},
 booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
 pages = {142--150},
 publisher = {Association for Computational Linguistics},
 title = {Learning Word Vectors for Sentiment Analysis},
 url = {https://aclanthology.org/P11-1015},
 year = {2011}
}

@inproceedings{mcauley2013hidden,
 author = {Julian J. McAuley and
Jure Leskovec},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/recsys/McAuleyL13.bib},
 booktitle = {Seventh {ACM} Conference on Recommender Systems, RecSys '13, Hong
Kong, China, October 12-16, 2013},
 doi = {10.1145/2507157.2507163},
 editor = {Qiang Yang and
Irwin King and
Qing Li and
Pearl Pu and
George Karypis},
 pages = {165--172},
 publisher = {{ACM}},
 timestamp = {Wed, 14 Nov 2018 00:00:00 +0100},
 title = {Hidden factors and hidden topics: understanding rating dimensions
with review text},
 url = {https://doi.org/10.1145/2507157.2507163},
 year = {2013}
}

@article{lee2014large,
 author = {Lee, Ching-Pei and Lin, Chih-Jen},
 journal = {Neural computation},
 number = {4},
 pages = {781--817},
 publisher = {MIT Press},
 title = {Large-scale linear ranksvm},
 volume = {26},
 year = {2014}
}

@article{sculley2009large,
 author = {Sculley, D},
 title = {Large scale learning to rank},
 year = {2009}
}

@article{sinoara2019knowledge,
 author = {Sinoara, Roberta A and Camacho-Collados, Jose and Rossi, Rafael G and Navigli, Roberto and Rezende, Solange O},
 journal = {Knowledge-Based Systems},
 pages = {955--971},
 publisher = {Elsevier},
 title = {Knowledge-enhanced document embeddings for text classification},
 volume = {163},
 year = {2019}
}

@inproceedings{zhang2019integrating,
 address = {Minneapolis, Minnesota},
 author = {Zhang, Jingqing  and
Lertvittayakumjorn, Piyawat  and
Guo, Yike},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1108},
 pages = {1031--1040},
 publisher = {Association for Computational Linguistics},
 title = {Integrating Semantic Knowledge to Tackle Zero-shot Text Classification},
 url = {https://aclanthology.org/N19-1108},
 year = {2019}
}

@inproceedings{chen2019deep,
 author = {Jindong Chen and
Yizhou Hu and
Jingping Liu and
Yanghua Xiao and
Haiyun Jiang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/ChenHLXJ19.bib},
 booktitle = {The Thirty-Third {AAAI} Conference on Artificial Intelligence, {AAAI}
2019, The Thirty-First Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2019, The Ninth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2019, Honolulu, Hawaii,
USA, January 27 - February 1, 2019},
 doi = {10.1609/aaai.v33i01.33016252},
 pages = {6252--6259},
 publisher = {{AAAI} Press},
 timestamp = {Wed, 25 Sep 2019 01:00:00 +0200},
 title = {Deep Short Text Classification with Knowledge Powered Attention},
 url = {https://doi.org/10.1609/aaai.v33i01.33016252},
 year = {2019}
}

@inproceedings{yang2019enhancing,
 address = {Florence, Italy},
 author = {Yang, An  and
Wang, Quan  and
Liu, Jing  and
Liu, Kai  and
Lyu, Yajuan  and
Wu, Hua  and
She, Qiaoqiao  and
Li, Sujian},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1226},
 pages = {2346--2357},
 publisher = {Association for Computational Linguistics},
 title = {Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension},
 url = {https://aclanthology.org/P19-1226},
 year = {2019}
}

@inproceedings{liu2020k,
 author = {Weijie Liu and
Peng Zhou and
Zhe Zhao and
Zhiruo Wang and
Qi Ju and
Haotang Deng and
Ping Wang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/LiuZ0WJD020.bib},
 booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
 pages = {2901--2908},
 publisher = {{AAAI} Press},
 timestamp = {Thu, 20 Aug 2020 01:00:00 +0200},
 title = {{K-BERT:} Enabling Language Representation with Knowledge Graph},
 url = {https://aaai.org/ojs/index.php/AAAI/article/view/5681},
 year = {2020}
}

@article{guan2020knowledge,
 author = {Guan, Jian  and
Huang, Fei  and
Zhao, Zhihao  and
Zhu, Xiaoyan  and
Huang, Minlie},
 doi = {10.1162/tacl_a_00302},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {93--108},
 title = {A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation},
 url = {https://aclanthology.org/2020.tacl-1.7},
 volume = {8},
 year = {2020}
}

@inproceedings{wang2019superglue,
 author = {Alex Wang and
Yada Pruksachatkun and
Nikita Nangia and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/WangPNSMHLB19.bib},
 booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
 editor = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
 pages = {3261--3275},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {SuperGLUE: {A} Stickier Benchmark for General-Purpose Language Understanding
Systems},
 url = {https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html},
 year = {2019}
}

@article{chen2021adaprompt,
 author = {Chen, Xiang and Xie, Xin and Zhang, Ningyu and Yan, Jiahuan and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Si, Luo and Chen, Huajun},
 journal = {ArXiv preprint},
 title = {Adaprompt: Adaptive prompt-based finetuning for relation extraction},
 url = {https://arxiv.org/abs/2104.07650},
 volume = {abs/2104.07650},
 year = {2021}
}

@article{han2021ptr,
 author = {Han, Xu and Zhao, Weilin and Ding, Ning and Liu, Zhiyuan and Sun, Maosong},
 journal = {ArXiv preprint},
 title = {PTR: Prompt Tuning with Rules for Text Classification},
 url = {https://arxiv.org/abs/2105.11259},
 volume = {abs/2105.11259},
 year = {2021}
}

@inproceedings{zhang2019ernie,
 address = {Florence, Italy},
 author = {Zhang, Zhengyan  and
Han, Xu  and
Liu, Zhiyuan  and
Jiang, Xin  and
Sun, Maosong  and
Liu, Qun},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1139},
 pages = {1441--1451},
 publisher = {Association for Computational Linguistics},
 title = {{ERNIE}: Enhanced Language Representation with Informative Entities},
 url = {https://aclanthology.org/P19-1139},
 year = {2019}
}

@inproceedings{lan2019albert,
 author = {Zhenzhong Lan and
Mingda Chen and
Sebastian Goodman and
Kevin Gimpel and
Piyush Sharma and
Radu Soricut},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/LanCGGSS20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
Representations},
 url = {https://openreview.net/forum?id=H1eA7AEtvS},
 year = {2020}
}

@article{liu2019roberta,
 author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
 journal = {ArXiv preprint},
 title = {Roberta: A robustly optimized bert pretraining approach},
 url = {https://arxiv.org/abs/1907.11692},
 volume = {abs/1907.11692},
 year = {2019}
}


@inproceedings{holtzman2021surface,
  title={Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right},
  author={Holtzman, Ari and West, Peter and Shwartz, Vered and Choi, Yejin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7038--7051},
  year={2021}
}

@inproceedings{shan2018confidence,
 author = {Shan, Yingchun and Bu, Chenyang and Liu, Xiaojian and Ji, Shengwei and Li, Lei},
 booktitle = {Proceedings of ICBK},
 organization = {IEEE},
 pages = {33--40},
 title = {Confidence-aware negative sampling method for noisy knowledge graph embedding},
 year = {2018}
}

@inproceedings{meng2020text,
 address = {Online},
 author = {Meng, Yu  and
Zhang, Yunyi  and
Huang, Jiaxin  and
Xiong, Chenyan  and
Ji, Heng  and
Zhang, Chao  and
Han, Jiawei},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.724},
 pages = {9006--9017},
 publisher = {Association for Computational Linguistics},
 title = {Text Classification Using Label Names Only: A Language Model Self-Training Approach},
 url = {https://aclanthology.org/2020.emnlp-main.724},
 year = {2020}
}

@inproceedings{chu2020unsupervised,
 address = {Online},
 author = {Chu, Zewei  and
Stratos, Karl  and
Gimpel, Kevin},
 booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
 doi = {10.18653/v1/2021.findings-acl.365},
 pages = {4165--4178},
 publisher = {Association for Computational Linguistics},
 title = {Unsupervised Label Refinement Improves Dataless Text Classification},
 url = {https://aclanthology.org/2021.findings-acl.365},
 year = {2021}
}

@inproceedings{schick2020automatically,   title={Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification},   author={Schick, Timo and Schmid, Helmut and Sch{\"u}tze, Hinrich},   booktitle={Proceedings of COLING},   pages={5569--5578},   year={2020} }

@inproceedings{schick2021s,
title={It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
author={Schick, Timo and Sch{\"u}tze, Hinrich},
booktitle={Proceedings of NAACL},
pages={2339--2352},
year={2021}
}

@article{liu2021gpt,
 author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
 journal = {ArXiv preprint},
 title = {GPT Understands, Too},
 url = {https://arxiv.org/abs/2103.10385},
 volume = {abs/2103.10385},
 year = {2021}
}



@inproceedings{yin2019benchmarking,
 address = {Hong Kong, China},
 author = {Yin, Wenpeng  and
Hay, Jamaal  and
Roth, Dan},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1404},
 pages = {3914--3923},
 publisher = {Association for Computational Linguistics},
 title = {Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach},
 url = {https://aclanthology.org/D19-1404},
 year = {2019}
}

@inproceedings{petroni2019language,
 address = {Hong Kong, China},
 author = {Petroni, Fabio  and
Rockt{\"a}schel, Tim  and
Riedel, Sebastian  and
Lewis, Patrick  and
Bakhtin, Anton  and
Wu, Yuxiang  and
Miller, Alexander},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1250},
 pages = {2463--2473},
 publisher = {Association for Computational Linguistics},
 title = {Language Models as Knowledge Bases?},
 url = {https://aclanthology.org/D19-1250},
 year = {2019}
}

@inproceedings{petroni2020how,
 author = {Fabio Petroni and Patrick Lewis and Aleksandra Piktus and Tim Rockt{\"a}schel and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},
 booktitle = {Automated Knowledge Base Construction},
 title = {How Context Affects Language Models' Factual Predictions},
 year = {2020}
}


@article{kowsari2019text,
 author = {Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
 journal = {Information},
 number = {4},
 pages = {150},
 publisher = {Multidisciplinary Digital Publishing Institute},
 title = {Text Classification Algorithms: A Survey},
 volume = {10},
 year = {2019}
}

@inproceedings{rajpurkar2016squad,
 address = {Austin, Texas},
 author = {Rajpurkar, Pranav  and
Zhang, Jian  and
Lopyrev, Konstantin  and
Liang, Percy},
 booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/D16-1264},
 pages = {2383--2392},
 publisher = {Association for Computational Linguistics},
 title = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},
 url = {https://aclanthology.org/D16-1264},
 year = {2016}
}

@inproceedings{devlin2019bert,
 address = {Minneapolis, Minnesota},
 author = {Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1423},
 pages = {4171--4186},
 publisher = {Association for Computational Linguistics},
 title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 url = {https://aclanthology.org/N19-1423},
 year = {2019}
}

@inproceedings{peters2018deep,
 address = {New Orleans, Louisiana},
 author = {Peters, Matthew E.  and
Neumann, Mark  and
Iyyer, Mohit  and
Gardner, Matt  and
Clark, Christopher  and
Lee, Kenton  and
Zettlemoyer, Luke},
 booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
 doi = {10.18653/v1/N18-1202},
 pages = {2227--2237},
 publisher = {Association for Computational Linguistics},
 title = {Deep Contextualized Word Representations},
 url = {https://aclanthology.org/N18-1202},
 year = {2018}
}

@article{radford2018improving,
 author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
 title = {Improving language understanding by generative pre-training},
 year = {2018}
}


@article{jiang2020can,
 author = {Jiang, Zhengbao  and
Xu, Frank F.  and
Araki, Jun  and
Neubig, Graham},
 doi = {10.1162/tacl_a_00324},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {423--438},
 title = {How Can We Know What Language Models Know?},
 url = {https://aclanthology.org/2020.tacl-1.28},
 volume = {8},
 year = {2020}
}


@inproceedings{schick2020s,
 address = {Online},
 author = {Schick, Timo  and
Sch{\"u}tze, Hinrich},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.185},
 pages = {2339--2352},
 publisher = {Association for Computational Linguistics},
 title = {It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
 url = {https://aclanthology.org/2021.naacl-main.185},
 year = {2021}
}

@article{xie2019unsupervised,
 author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V},
 journal = {ArXiv preprint},
 title = {Unsupervised data augmentation for consistency training},
 url = {https://arxiv.org/abs/1904.12848},
 volume = {abs/1904.12848},
 year = {2019}
}



@InProceedings{pmlr-v139-zhao21c,
  title = 	 {Calibrate Before Use: Improving Few-shot Performance of Language Models},
  author =       {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12697--12706},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zhao21c.html},
  abstract = 	 {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model’s bias towards each answer by asking for its prediction when given a training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2’s accuracy (up to 30.0% absolute) across different choices of the prompt, while also making learning considerably more stable.}
}



@inproceedings{shin2020autoprompt,
 address = {Online},
 author = {Shin, Taylor  and
Razeghi, Yasaman  and
Logan IV, Robert L.  and
Wallace, Eric  and
Singh, Sameer},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.346},
 pages = {4222--4235},
 publisher = {Association for Computational Linguistics},
 title = {{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts},
 url = {https://aclanthology.org/2020.emnlp-main.346},
 year = {2020}
}

@inproceedings{schick2020exploiting,
 address = {Online},
 author = {Schick, Timo  and
Sch{\"u}tze, Hinrich},
 booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
 pages = {255--269},
 publisher = {Association for Computational Linguistics},
 title = {Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
 url = {https://aclanthology.org/2021.eacl-main.20},
 year = {2021}
}

@inproceedings{gao2020making,
 address = {Online},
 author = {Gao, Tianyu  and
Fisch, Adam  and
Chen, Danqi},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.295},
 pages = {3816--3830},
 publisher = {Association for Computational Linguistics},
 title = {Making Pre-trained Language Models Better Few-shot Learners},
 url = {https://aclanthology.org/2021.acl-long.295},
 year = {2021}
}

@book{Aho:72,
 address = {Englewood Cliffs, NJ},
 author = {Alfred V. Aho and Jeffrey D. Ullman},
 publisher = {Prentice-Hall},
 title = {The Theory of Parsing, Translation and Compiling},
 volume = {1},
 year = {1972}
}

@book{APA:83,
 address = {Washington, DC},
 author = {{American Psychological Association}},
 publisher = {American Psychological Association},
 title = {Publications Manual},
 year = {1983}
}

@article{Chandra:81,
 author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
 doi = {10.1145/322234.322243},
 journal = {Journal of the Association for Computing Machinery},
 number = {1},
 pages = {114--133},
 title = {Alternation},
 volume = {28},
 year = {1981}
}

@inproceedings{andrew2007scalable,
 author = {Andrew, Galen and Gao, Jianfeng},
 booktitle = {Proceedings of ICML},
 pages = {33--40},
 title = {Scalable training of {L1}-regularized log-linear models},
 year = {2007}
}

@book{Gusfield:97,
 address = {Cambridge, UK},
 author = {Dan Gusfield},
 publisher = {Cambridge University Press},
 title = {Algorithms on Strings, Trees and Sequences},
 year = {1997}
}

@article{rasooli-tetrault-2015,
 author = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
 journal = {ArXiv preprint},
 title = {Yara Parser: {A} Fast and Accurate Dependency Parser},
 url = {https://arxiv.org/abs/1503.06733},
 volume = {abs/1503.06733},
 year = {2015}
}

@article{Ando2005,
 acmid = {1194905},
 author = {Ando, Rie Kubota and Zhang, Tong},
 issn = {1532-4435},
 issue_date = {12/1/2005},
 journal = {Journal of Machine Learning Research},
 numpages = {37},
 pages = {1817--1853},
 publisher = {JMLR.org},
 title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
 volume = {6},
 year = {2005}
}

@article{xu2021pre,
 author = {Xu, Han and Zhengyan, Zhang and Ning, Ding and Yuxian, Gu and Xiao, Liu and Yuqi, Huo and Jiezhong, Qiu and Liang, Zhang and Wentao, Han and Minlie, Huang and others},
 journal = {ArXiv preprint},
 title = {Pre-Trained Models: Past, Present and Future},
 url = {https://arxiv.org/abs/2106.07139},
 volume = {abs/2106.07139},
 year = {2021}
}


@book{Aho:72,
 address = {Englewood Cliffs, NJ},
 author = {Alfred V. Aho and Jeffrey D. Ullman},
 publisher = {Prentice-Hall},
 title = {The Theory of Parsing, Translation and Compiling},
 volume = {1},
 year = {1972}
}

@book{APA:83,
 address = {Washington, DC},
 author = {{American Psychological Association}},
 publisher = {American Psychological Association},
 title = {Publications Manual},
 year = {1983}
}

@article{Chandra:81,
 author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
 doi = {10.1145/322234.322243},
 journal = {Journal of the Association for Computing Machinery},
 number = {1},
 pages = {114--133},
 title = {Alternation},
 volume = {28},
 year = {1981}
}

@inproceedings{andrew2007scalable,
 author = {Andrew, Galen and Gao, Jianfeng},
 booktitle = {Proceedings of the 24th International Conference on Machine Learning},
 pages = {33--40},
 title = {Scalable training of {$L_1$}-regularized log-linear models},
 url = {https://dl.acm.org/doi/abs/10.1145/1273496.1273501},
 year = {2007}
}

@book{Gusfield:97,
 address = {Cambridge, UK},
 author = {Dan Gusfield},
 publisher = {Cambridge University Press},
 title = {Algorithms on Strings, Trees and Sequences},
 url = {https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3},
 year = {1997}
}

@article{rasooli-tetrault-2015,
 author = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
 journal = {ArXiv preprint},
 title = {Yara Parser: {A} Fast and Accurate Dependency Parser},
 url = {https://arxiv.org/abs/1503.06733},
 volume = {abs/1503.06733},
 year = {2015}
}

@article{Ando2005,
 acmid = {1194905},
 author = {Ando, Rie Kubota and Zhang, Tong},
 issn = {1532-4435},
 issue_date = {12/1/2005},
 journal = {Journal of Machine Learning Research},
 numpages = {37},
 pages = {1817--1853},
 publisher = {JMLR.org},
 title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
 url = {https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf},
 volume = {6},
 year = {2005}
}

@article{ct1965,
 author = {Cooley, James W. and Tukey, John W.},
 journal = {Mathematics of Computation},
 number = {90},
 pages = {297--301},
 title = {An algorithm for the machine calculation of complex {F}ourier series},
 url = {https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf},
 volume = {19},
 year = {1965}
}


@article{turing1950computing,
 author = {Turing, Alan M and Haugeland, J},
 journal = {The Turing Test: Verbal Behavior as the Hallmark of Intelligence},
 pages = {29--56},
 title = {Computing machinery and intelligence},
 year = {1950}
}

@article{xie2021explanation,
 author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
 journal = {ArXiv preprint},
 title = {An explanation of in-context learning as implicit bayesian inference},
 url = {https://arxiv.org/abs/2111.02080},
 volume = {abs/2111.02080},
 year = {2021}
}

@article{lieber2021jurassic,
 author = {Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},
 journal = {White Paper. AI21 Labs},
 title = {Jurassic-1: Technical details and evaluation},
 volume = {1},
 year = {2021}
}

@article{levine2022standing,
 author = {Levine, Yoav and Dalmedigos, Itay and Ram, Ori and Zeldes, Yoel and Jannai, Daniel and Muhlgay, Dor and Osin, Yoni and Lieber, Opher and Lenz, Barak and Shalev-Shwartz, Shai and others},
 journal = {ArXiv preprint},
 title = {Standing on the shoulders of giant frozen language models},
 url = {https://arxiv.org/abs/2204.10019},
 volume = {abs/2204.10019},
 year = {2022}
}

@article{10.1145/3560815,
 abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website including constantly updated survey and paperlist.},
 address = {New York, NY, USA},
 articleno = {195},
 author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
 doi = {10.1145/3560815},
 issn = {0360-0300},
 issue_date = {September 2023},
 journal = {ACM Comput. Surv.},
 keywords = {Pre-trained language models, prompting},
 number = {9},
 numpages = {35},
 publisher = {Association for Computing Machinery},
 title = {Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
 url = {https://doi.org/10.1145/3560815},
 volume = {55},
 year = {2023}
}


@article{bhakthavatsalam2021think,
 author = {Bhakthavatsalam, Sumithra and Khashabi, Daniel and Khot, Tushar and Mishra, Bhavana Dalvi and Richardson, Kyle and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind and Clark, Peter},
 journal = {ArXiv preprint},
 title = {Think you have solved direct-answer question answering? Try ARC-DA, the direct-answer AI2 reasoning challenge},
 url = {https://arxiv.org/abs/2102.03315},
 volume = {abs/2102.03315},
 year = {2021}
}

@article{chaudhry2019tiny,
 author = {Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip HS and Ranzato, Marc'Aurelio},
 journal = {ArXiv preprint},
 title = {On tiny episodic memories in continual learning},
 url = {https://arxiv.org/abs/1902.10486},
 volume = {abs/1902.10486},
 year = {2019}
}

@article{kenton2021alignment,
 author = {Kenton, Zachary and Everitt, Tom and Weidinger, Laura and Gabriel, Iason and Mikulik, Vladimir and Irving, Geoffrey},
 journal = {ArXiv preprint},
 title = {Alignment of language agents},
 url = {https://arxiv.org/abs/2103.14659},
 volume = {abs/2103.14659},
 year = {2021}
}

@article{kwiatkowski2019natural,
 address = {Cambridge, MA},
 author = {Kwiatkowski, Tom  and
Palomaki, Jennimaria  and
Redfield, Olivia  and
Collins, Michael  and
Parikh, Ankur  and
Alberti, Chris  and
Epstein, Danielle  and
Polosukhin, Illia  and
Devlin, Jacob  and
Lee, Kenton  and
Toutanova, Kristina  and
Jones, Llion  and
Kelcey, Matthew  and
Chang, Ming-Wei  and
Dai, Andrew M.  and
Uszkoreit, Jakob  and
Le, Quoc  and
Petrov, Slav},
 doi = {10.1162/tacl_a_00276},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {452--466},
 publisher = {MIT Press},
 title = {Natural Questions: A Benchmark for Question Answering Research},
 url = {https://aclanthology.org/Q19-1026},
 volume = {7},
 year = {2019}
}

@article{min2022crepe,
 author = {Min, Sewon and Zettlemoyer, Luke and Hajishirzi, Hannaneh and others},
 journal = {arXiv e-prints},
 pages = {arXiv--2211},
 title = {CREPE: Open-Domain Question Answering with False Presuppositions},
 year = {2022}
}

@article{kaplan1978indirect,
 author = {Kaplan, S. Jerrold},
 journal = {American Journal of Computational Linguistics},
 note = {Microfiche 80},
 pages = {21--28},
 title = {Indirect Responses to Loaded Questions},
 url = {https://aclanthology.org/J78-3037},
 year = {1978}
}

@article{du2022shortcut,
 author = {Du, Mengnan and He, Fengxiang and Zou, Na and Tao, Dacheng and Hu, Xia},
 journal = {ArXiv preprint},
 title = {Shortcut learning of large language models in natural language understanding: A survey},
 url = {https://arxiv.org/abs/2208.11857},
 volume = {abs/2208.11857},
 year = {2022}
}

@article{10.1093/mind/LIX.236.433,
 author = {TURING, A. M.},
 doi = {10.1093/mind/LIX.236.433},
 eprint = {https://academic.oup.com/mind/article-pdf/LIX/236/433/30123314/lix-236-433.pdf},
 issn = {0026-4423},
 journal = {Mind},
 number = {236},
 pages = {433-460},
 title = {{I.—COMPUTING MACHINERY AND INTELLIGENCE}},
 url = {https://doi.org/10.1093/mind/LIX.236.433},
 volume = {LIX},
 year = {1950}
}

@article{han2021pretrained,
 author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and others},
 journal = {AI Open},
 pages = {225--250},
 publisher = {Elsevier},
 title = {Pre-trained models: Past, present and future},
 volume = {2},
 year = {2021}
}

@article{tafjord2021general,
 author = {Tafjord, Oyvind and Clark, Peter},
 journal = {ArXiv preprint},
 title = {General-purpose question-answering with macaw},
 url = {https://arxiv.org/abs/2109.02593},
 volume = {abs/2109.02593},
 year = {2021}
}

@article{DBLP:journals/corr/abs-1801-00631,
 author = {Gary Marcus},
 journal = {ArXiv preprint},
 title = {Deep Learning: {A} Critical Appraisal},
 url = {https://arxiv.org/abs/1801.00631},
 volume = {abs/1801.00631},
 year = {2018}
}

@article{davis2015commonsense,
 author = {Davis, Ernest and Marcus, Gary},
 journal = {Communications of the ACM},
 number = {9},
 pages = {92--103},
 publisher = {ACM New York, NY, USA},
 title = {Commonsense reasoning and commonsense knowledge in artificial intelligence},
 volume = {58},
 year = {2015}
}

@inproceedings{lin-etal-2020-birds,
 address = {Online},
 author = {Lin, Bill Yuchen  and
Lee, Seyeon  and
Khanna, Rahul  and
Ren, Xiang},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.557},
 pages = {6862--6868},
 publisher = {Association for Computational Linguistics},
 title = {{B}irds have four legs?! {N}umer{S}ense: {P}robing {N}umerical {C}ommonsense {K}nowledge of {P}re-{T}rained {L}anguage {M}odels},
 url = {https://aclanthology.org/2020.emnlp-main.557},
 year = {2020}
}

@article{ba2016layer,
 author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
 journal = {ArXiv preprint},
 title = {Layer normalization},
 url = {https://arxiv.org/abs/1607.06450},
 volume = {abs/1607.06450},
 year = {2016}
}

@inproceedings{he2016deep,
 author = {Kaiming He and
Xiangyu Zhang and
Shaoqing Ren and
Jian Sun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
 booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
 doi = {10.1109/CVPR.2016.90},
 pages = {770--778},
 publisher = {{IEEE} Computer Society},
 timestamp = {Wed, 17 Apr 2019 01:00:00 +0200},
 title = {Deep Residual Learning for Image Recognition},
 url = {https://doi.org/10.1109/CVPR.2016.90},
 year = {2016}
}

@inproceedings{vaswani2017attention,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2017}
}

@inproceedings{kenton2019bert,
 address = {Minneapolis, Minnesota},
 author = {Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1423},
 pages = {4171--4186},
 publisher = {Association for Computational Linguistics},
 title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 url = {https://aclanthology.org/N19-1423},
 year = {2019}
}

@inproceedings{peters2018deep,
 address = {New Orleans, Louisiana},
 author = {Peters, Matthew E.  and
Neumann, Mark  and
Iyyer, Mohit  and
Gardner, Matt  and
Clark, Christopher  and
Lee, Kenton  and
Zettlemoyer, Luke},
 booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
 doi = {10.18653/v1/N18-1202},
 pages = {2227--2237},
 publisher = {Association for Computational Linguistics},
 title = {Deep Contextualized Word Representations},
 url = {https://aclanthology.org/N18-1202},
 year = {2018}
}



@article{chen2019uniter,
 author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
 title = {Uniter: Learning universal image-text representations},
 year = {2019}
}

@inproceedings{radford2021learning,
 author = {Alec Radford and
Jong Wook Kim and
Chris Hallacy and
Aditya Ramesh and
Gabriel Goh and
Sandhini Agarwal and
Girish Sastry and
Amanda Askell and
Pamela Mishkin and
Jack Clark and
Gretchen Krueger and
Ilya Sutskever},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/RadfordKHRGASAM21.bib},
 booktitle = {Proceedings of the 38th International Conference on Machine Learning,
{ICML} 2021, 18-24 July 2021, Virtual Event},
 editor = {Marina Meila and
Tong Zhang},
 pages = {8748--8763},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Wed, 25 Aug 2021 01:00:00 +0200},
 title = {Learning Transferable Visual Models From Natural Language Supervision},
 url = {http://proceedings.mlr.press/v139/radford21a.html},
 volume = {139},
 year = {2021}
}


@article{sanh2021multitask,
 author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
 journal = {ArXiv preprint},
 title = {Multitask prompted training enables zero-shot task generalization},
 url = {https://arxiv.org/abs/2110.08207},
 volume = {abs/2110.08207},
 year = {2021}
}

@article{zhang2022opt,
 author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
 journal = {ArXiv preprint},
 title = {Opt: Open pre-trained transformer language models},
 url = {https://arxiv.org/abs/2205.01068},
 volume = {abs/2205.01068},
 year = {2022}
}

@inproceedings{talmor2018commonsenseqa,
 address = {Minneapolis, Minnesota},
 author = {Talmor, Alon  and
Herzig, Jonathan  and
Lourie, Nicholas  and
Berant, Jonathan},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1421},
 pages = {4149--4158},
 publisher = {Association for Computational Linguistics},
 title = {{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge},
 url = {https://aclanthology.org/N19-1421},
 year = {2019}
}

@inproceedings{sap2019socialiqa,
 address = {Hong Kong, China},
 author = {Sap, Maarten  and
Rashkin, Hannah  and
Chen, Derek  and
Le Bras, Ronan  and
Choi, Yejin},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1454},
 pages = {4463--4473},
 publisher = {Association for Computational Linguistics},
 title = {Social {IQ}a: Commonsense Reasoning about Social Interactions},
 url = {https://aclanthology.org/D19-1454},
 year = {2019}
}

@inproceedings{bisk2020piqa,
 author = {Yonatan Bisk and
Rowan Zellers and
Ronan LeBras and
Jianfeng Gao and
Yejin Choi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
 booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
 pages = {7432--7439},
 publisher = {{AAAI} Press},
 timestamp = {Thu, 04 Jun 2020 01:00:00 +0200},
 title = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
 url = {https://aaai.org/ojs/index.php/AAAI/article/view/6239},
 year = {2020}
}


@inproceedings{yin2016simple,
 address = {Osaka, Japan},
 author = {Yin, Wenpeng  and
Yu, Mo  and
Xiang, Bing  and
Zhou, Bowen  and
Sch{\"u}tze, Hinrich},
 booktitle = {Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
 pages = {1746--1756},
 publisher = {The COLING 2016 Organizing Committee},
 title = {Simple Question Answering by Attentive Convolutional Neural Network},
 url = {https://aclanthology.org/C16-1164},
 year = {2016}
}

@inproceedings{papineni2002bleu,
 address = {Philadelphia, Pennsylvania, USA},
 author = {Papineni, Kishore  and
Roukos, Salim  and
Ward, Todd  and
Zhu, Wei-Jing},
 booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.3115/1073083.1073135},
 pages = {311--318},
 publisher = {Association for Computational Linguistics},
 title = {{B}leu: a Method for Automatic Evaluation of Machine Translation},
 url = {https://aclanthology.org/P02-1040},
 year = {2002}
}

@inproceedings{lin2004rouge,
 address = {Barcelona, Spain},
 author = {Lin, Chin-Yew},
 booktitle = {Text Summarization Branches Out},
 pages = {74--81},
 publisher = {Association for Computational Linguistics},
 title = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
 url = {https://aclanthology.org/W04-1013},
 year = {2004}
}

@article{nakano2021webgpt,
 author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
 journal = {ArXiv preprint},
 title = {WebGPT: Browser-assisted question-answering with human feedback},
 url = {https://arxiv.org/abs/2112.09332},
 volume = {abs/2112.09332},
 year = {2021}
}

@inproceedings{zhou2020evaluating,
 author = {Xuhui Zhou and
Yue Zhang and
Leyang Cui and
Dandan Huang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/ZhouZCH20.bib},
 booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
 pages = {9733--9740},
 publisher = {{AAAI} Press},
 timestamp = {Tue, 02 Feb 2021 00:00:00 +0100},
 title = {Evaluating Commonsense in Pre-Trained Language Models},
 url = {https://aaai.org/ojs/index.php/AAAI/article/view/6523},
 year = {2020}
}

@inproceedings{lin2021truthfulqa,
 address = {Dublin, Ireland},
 author = {Lin, Stephanie  and
Hilton, Jacob  and
Evans, Owain},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.229},
 pages = {3214--3252},
 publisher = {Association for Computational Linguistics},
 title = {{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods},
 url = {https://aclanthology.org/2022.acl-long.229},
 year = {2022}
}

@inproceedings{khashabi2021gooaq,
 address = {Punta Cana, Dominican Republic},
 author = {Khashabi, Daniel  and
Ng, Amos  and
Khot, Tushar  and
Sabharwal, Ashish  and
Hajishirzi, Hannaneh  and
Callison-Burch, Chris},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
 doi = {10.18653/v1/2021.findings-emnlp.38},
 pages = {421--433},
 publisher = {Association for Computational Linguistics},
 title = {{G}oo{AQ}: Open Question Answering with Diverse Answer Types},
 url = {https://aclanthology.org/2021.findings-emnlp.38},
 year = {2021}
}

@inproceedings{fan2019eli5,
 address = {Florence, Italy},
 author = {Fan, Angela  and
Jernite, Yacine  and
Perez, Ethan  and
Grangier, David  and
Weston, Jason  and
Auli, Michael},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1346},
 pages = {3558--3567},
 publisher = {Association for Computational Linguistics},
 title = {{ELI}5: Long Form Question Answering},
 url = {https://aclanthology.org/P19-1346},
 year = {2019}
}



@article{bhakthavatsalam2020genericskb,
 author = {Bhakthavatsalam, Sumithra and Anastasiades, Chloe and Clark, Peter},
 journal = {ArXiv preprint},
 title = {Genericskb: A knowledge base of generic statements},
 url = {https://arxiv.org/abs/2005.00660},
 volume = {abs/2005.00660},
 year = {2020}
}

@inproceedings{mihaylov2018can,
 address = {Brussels, Belgium},
 author = {Mihaylov, Todor  and
Clark, Peter  and
Khot, Tushar  and
Sabharwal, Ashish},
 booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/D18-1260},
 pages = {2381--2391},
 publisher = {Association for Computational Linguistics},
 title = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 url = {https://aclanthology.org/D18-1260},
 year = {2018}
}

@inproceedings{raina2022answer,
 address = {Dublin, Ireland},
 author = {Raina, Vatsal  and
Gales, Mark},
 booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
 doi = {10.18653/v1/2022.findings-acl.82},
 pages = {1020--1034},
 publisher = {Association for Computational Linguistics},
 title = {Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension},
 url = {https://aclanthology.org/2022.findings-acl.82},
 year = {2022}
}


@inproceedings{yen2021unanswerable,
 author = {Yen, An-Zi and Huang, Hen-Hsen and Chen, Hsin-Hsi},
 booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {16},
 pages = {14266--14275},
 title = {Unanswerable question correction in question answering over personal knowledge base},
 volume = {35},
 year = {2021}
}

@article{davis2020unanswerable,
 author = {Davis, Ernest},
 journal = {Frontiers in Artificial Intelligence},
 pages = {51},
 publisher = {Frontiers Media SA},
 title = {Unanswerable questions about images and texts},
 volume = {3},
 year = {2020}
}

@inproceedings{antol2015vqa,
 author = {Stanislaw Antol and
Aishwarya Agrawal and
Jiasen Lu and
Margaret Mitchell and
Dhruv Batra and
C. Lawrence Zitnick and
Devi Parikh},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iccv/AntolALMBZP15.bib},
 booktitle = {2015 {IEEE} International Conference on Computer Vision, {ICCV} 2015,
Santiago, Chile, December 7-13, 2015},
 doi = {10.1109/ICCV.2015.279},
 pages = {2425--2433},
 publisher = {{IEEE} Computer Society},
 timestamp = {Wed, 24 May 2017 01:00:00 +0200},
 title = {{VQA:} Visual Question Answering},
 url = {https://doi.org/10.1109/ICCV.2015.279},
 year = {2015}
}


@article{wei2021finetuned,
 author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
 journal = {ArXiv preprint},
 title = {Finetuned language models are zero-shot learners},
 url = {https://arxiv.org/abs/2109.01652},
 volume = {abs/2109.01652},
 year = {2021}
}

@inproceedings{ye2021crossfit,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Ye, Qinyuan  and
Lin, Bill Yuchen  and
Ren, Xiang},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.572},
 pages = {7163--7189},
 publisher = {Association for Computational Linguistics},
 title = {{C}ross{F}it: A Few-shot Learning Challenge for Cross-task Generalization in {NLP}},
 url = {https://aclanthology.org/2021.emnlp-main.572},
 year = {2021}
}


@inproceedings{hu2021knowledgeable,
 address = {Dublin, Ireland},
 author = {Hu, Shengding  and
Ding, Ning  and
Wang, Huadong  and
Liu, Zhiyuan  and
Wang, Jingang  and
Li, Juanzi  and
Wu, Wei  and
Sun, Maosong},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.158},
 pages = {2225--2240},
 publisher = {Association for Computational Linguistics},
 title = {Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification},
 url = {https://aclanthology.org/2022.acl-long.158},
 year = {2022}
}

@article{li2021pretrained,
 author = {Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Wen, Ji-Rong},
 journal = {ArXiv preprint},
 title = {Pretrained language models for text generation: A survey},
 url = {https://arxiv.org/abs/2105.10311},
 volume = {abs/2105.10311},
 year = {2021}
}

@inproceedings{guu2020retrieval,
 author = {Kelvin Guu and
Kenton Lee and
Zora Tung and
Panupong Pasupat and
Ming{-}Wei Chang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/GuuLTPC20.bib},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
 pages = {3929--3938},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 15 Dec 2020 00:00:00 +0100},
 title = {Retrieval Augmented Language Model Pre-Training},
 url = {http://proceedings.mlr.press/v119/guu20a.html},
 volume = {119},
 year = {2020}
}

@article{qin2021exploring,
 author = {Qin, Yujia and Wang, Xiaozhi and Su, Yusheng and Lin, Yankai and Ding, Ning and Liu, Zhiyuan and Li, Juanzi and Hou, Lei and Li, Peng and Sun, Maosong and others},
 journal = {ArXiv preprint},
 title = {Exploring low-dimensional intrinsic task subspace via prompt tuning},
 url = {https://arxiv.org/abs/2110.07867},
 volume = {abs/2110.07867},
 year = {2021}
}

@article{ding2022delta,
 author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
 journal = {ArXiv preprint},
 title = {Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models},
 url = {https://arxiv.org/abs/2203.06904},
 volume = {abs/2203.06904},
 year = {2022}
}

@article{srivastava2022beyond,
 author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
 journal = {ArXiv preprint},
 title = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
 url = {https://arxiv.org/abs/2206.04615},
 volume = {abs/2206.04615},
 year = {2022}
}

@article{scao2022bloom,
 author = {Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
 journal = {ArXiv preprint},
 title = {Bloom: A 176b-parameter open-access multilingual language model},
 url = {https://arxiv.org/abs/2211.05100},
 volume = {abs/2211.05100},
 year = {2022}
}


@misc{chatgpt,
 author = {OpenAI},
 title = {ChatGPT: Optimizing Language Models for Dialogue},
 url = {https://openai.com/blog/chatgpt/},
 year = {2022}
}


@misc{ALPAC,
  author = {Automatic Language Processing Advisory Committee},
  title = {A Report by the Automatic Language Processing Advisory Committee},
  url = {https://web.archive.org/web/20110409070141/http://www.mt-archive.info/ALPAC-1966.pdf},
  year = {2020}
}


@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas},
  journal={arXiv preprint arXiv:1301.3781},
  volume={3781},
  year={2013}
}

@article{wen2024understanding,
  title={Understanding warmup-stable-decay learning rates: A river valley loss landscape perspective},
  author={Wen, Kaiyue and Li, Zhiyuan and Wang, Jason and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2410.05192},
  year={2024}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}