% !TeX root = ../thuthesis-example.tex

\begin{resume}

  \section*{个人简历}

  1998 年 5 月 26 日出生于安徽省黄山市休宁县。

  2016 年 9 月考入清华大学物理系数理基础科学专业，2020 年 7 月本科毕业并获得理学学士学位。

  2020年9月免试进入清华大学计算机科学与技术系攻读计算机硕士。

  2023年9月经过硕转博考核，转为继续在清华大学计算机科学与技术系攻读博士学位至今。


  \section*{在学期间完成的相关学术成果}

  \subsection*{学术论文}

  \begin{achievements}
    % \item \cite{hu2024minicpm, hu2024predicting, hu-etal-2023-wont,hu-etal-2022-knowledgeable, NEURIPS2022_4027fc45, hu-etal-2023-opendelta,NEURIPS2020_73740ea8,huang2024unified,ding-etal-2022-openprompt,zhou-etal-2021-kacc,zhang-etal-2024-bench,zhao-etal-2024-decoratelm,luo2025a,zhang-etal-2024-beyond,he-etal-2024-ultraeval,he-etal-2024-olympiadbench,song-etal-2025-prosparse,ZHOU202057,cheng-etal-2024-legent,cui-etal-2022-prototypical,10.1145/3704435,peng-etal-2022-copen,chen-etal-2023-exploring,ding-etal-2023-enhancing,ding2023parameter,su-etal-2023-exploring}.
      
    \item \textbf{Hu S}, Tu Y, Han X, et al. MiniCPM: Unveiling the potential of small language models with
    scalable training strategies[C]//First Conference on Language Modeling. 2024.
      \item \textbf{Hu S}, Liu X, Han X, et al. Predicting emergent abilities with infinite resolution evaluation[C]//
      The Twelfth International Conference on Learning Representations. 2024.
      \item \textbf{Hu S}, Luo Y, Wang H, et al. Won‘t get fooled again: Answering questions with false premises
      [C/OL]//Rogers A, Boyd-Graber J, Okazaki N. Proceedings of the 61st Annual Meeting of
      the Association for Computational Linguistics. 2023: 5626-5643. DOI: 10.18653/v1/2023.acl-lon
      g.309.
      \item \textbf{Hu S}, Ding N, Wang H, et al. Knowledgeable prompt-tuning: Incorporating knowledge into
      prompt verbalizer for text classification[C/OL]//Proceedings of the 60th Annual Meeting of the
      Association for Computational Linguistics. 2022: 2225-2240. 
      \item \textbf{Hu S}*, Zhang Z*, Ding N, et al. Sparse structure search for delta tuning[C]//Koyejo S, Mohamed
      S, Agarwal A, et al. Advances in Neural Information Processing Systems: Vol. 35. Curran
      Associates, Inc., 2022: 9853-9865.
      \item \textbf{Hu S}, Ding N, Zhao W, et al. OpenDelta: A plug-and-play library for parameter-efficient adap-
      tation of Pretrained models[C/OL]//Bollegala D, Huang R, Ritter A. Proceedings of the 61st
      Annual Meeting of the Association for Computational Linguistics. 2023: 274-281. DOI:
      10.18653/v1/2023.acl-demo.26.
      \item \textbf{Hu S}, Xiong Z, Qu M, et al. Graph policy network for transferable active learning on graphs
      [C]//Larochelle H, Ranzato M, Hadsell R, et al. Advances in Neural Information Processing
      Systems: Vol. 33. Curran Associates, Inc., 2020: 10174-10185.
      \item Huang Y, \textbf{Hu S}, Han X, et al. Unified view of grokking, double descent and emergent abilities:
      A comprehensive study on algorithm task[C]//First Conference on Language Modeling. 2024.
      \item Ding N*, \textbf{Hu S}*, Zhao W*, et al. OpenPrompt: An open-source framework for prompt-learning
      [C/OL]//Proceedings of the 60th Annual Meeting of the Association for Computational Lin-
      guistics: System Demonstrations. DOI: 10.18653/v1/2022.acl-demo.10.
      \item Zhou J*, \textbf{Hu S}*, Lv X, et al. KACC: A multi-task benchmark for knowledge abstraction, con-
      cretization and completion[C/OL]//Findings of the Association for Computational Linguistics:
      ACL-IJCNLP 2021. DOI: 10.18653/v1/2021.findings-acl.153.
      \item Cui G, \textbf{Hu S}, Ding N, et al. Prototypical verbalizer for prompt-based few-shot tuning[C/OL]//
      Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. 2022: 7014-
      7024. https://aclanthology.org/2022.acl-long.483. DOI: 10.18653/v1/2022.acl-long.483.
      \item Qin Y, \textbf{Hu S}, Lin Y, et al. Tool learning with foundation models[J/OL]. ACM Comput. Surv.,
      2024, 57(4). DOI: 10.1145/3704435.
      \item Zhou J*, Cui G*, \textbf{Hu S}, et al. Graph neural networks: A review of methods and applications[J].
      AI Open, 2020, 1: 57-81.
      \item Zhang X, Chen Y, \textbf{Hu S}, et al. $\infty$Bench: Extending long context evaluation beyond 100K
      tokens[C/OL]//Ku L W, Martins A, Srikumar V. Proceedings of the 62nd Annual Meeting of
      the Association for Computational Linguistics. 2024: 15262-15277. DOI: 10.18653/v1/2024.acl-l
      ong.814.
      \item Luo K, Wen H, \textbf{Hu S}, et al. A multi-power law for loss curve prediction across learning rate
      schedules[C]//The Thirteenth International Conference on Learning Representations. 2025.
      \item Zhang X, Chen Y, \textbf{Hu S}, et al. Beyond the turn-based game: Enabling real-time conversations
      with duplex models[C/OL]//Al-Onaizan Y, Bansal M, Chen Y N. Proceedings of the 2024
      Conference on Empirical Methods in Natural Language Processing. 2024: 11543-11557. DOI: 10.18653/v1/2024.emnlp-main.644.
      \item He C, Luo R, \textbf{Hu S}, et al. UltraEval: A lightweight platform for flexible and comprehen-
      sive evaluation for LLMs[C/OL]//Cao Y, Feng Y, Xiong D. Proceedings of the 62nd An-
      nual Meeting of the Association for Computational Linguistics. 2024: 247-257. DOI: 10.18653/v1/2024.acl-demos.23.
      \item Peng H, Wang X, \textbf{Hu S}, et al. COPEN: Probing conceptual knowledge in Pretrained language
      models[C/OL]//Goldberg Y, Kozareva Z, Zhang Y. Proceedings of the 2022 Conference on
      Empirical Methods in Natural Language Processing. 2022: 5015-5035. DOI: 10.18653/v1/2022.emnlp-mai
      n.335.
      \item Zhao R, Thai Z L, Zhang Y, et al. DecorateLM: Data engineering through corpus rating, tagging,
      and editing with language models[C/OL]//Al-Onaizan Y, Bansal M, Chen Y N. Proceedings of
      the 2024 Conference on Empirical Methods in Natural Language Processing. 2024: 1401-1418. DOI: 10.18653/v1/2024.e
      mnlp-main.83.
      \item He C, Luo R, Bai Y, et al. OlympiadBench: A challenging benchmark for promoting AGI with
      olympiad-level bilingual multimodal scientific problems[C/OL]//Ku L W, Martins A, Srikumar
      V. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics.
      2024: 3828-3850. DOI: 10.18653/v1/2024.acl-long.211.
      \item Song C, Han X, Zhang Z, et al. ProSparse: Introducing and enhancing intrinsic activation
      sparsity within large language models[C]//Rambow O, Wanner L, Apidianaki M, et al. Pro-
      ceedings of the 31st International Conference on Computational Linguistics. 2025: 2626-2644.
      \item Cheng Z, Wang Z, Hu J, et al. LEGENT: Open platform for embodied agents[C/OL]//Cao Y,
      Feng Y, Xiong D. Proceedings of the 62nd Annual Meeting of the Association for Computa-
      tional Linguistics. 2024: 335-345. DOI: 10.18653/v1/2024.acl-demos.32.
      \item Chen Y, Ding N, Wang X, et al. Exploring lottery prompts for Pretrained language models
      [C/OL]//Rogers A, Boyd-Graber J, Okazaki N. Proceedings of the 61st Annual Meeting of
      the Association for Computational Linguistics. 2023: 15428-15444. DOI: 10.18653/v1/2023.acl-l
      ong.860.
      \item Ding N, Chen Y, Xu B, et al. Enhancing chat language models by scaling high-quality instruc-
      tional conversations[C/OL]//Bouamor H, Pino J, Bali K. Proceedings of the 2023 Conference
      on Empirical Methods in Natural Language Processing. 2023: 3029-3051. DOI: 10.18653/v1/2023.emnlp-main.183.
      \item Ding N, Qin Y, Yang G, et al. Parameter-efficient fine-tuning of large-scale Pretrained language
      models[J]. Nature Machine Intelligence, 2023, 5(3): 220-235.
      \item Su Y, Chan C M, Cheng J, et al. Exploring the impact of model scaling on parameter-efficient
      tuning[C/OL]//Bouamor H, Pino J, Bali K. Proceedings of the 2023 Conference on Empirical
      Methods in Natural Language Processing. 2023: 15062-15078. DOI: 10.18653/v1/2023.emnlp-main.931.      
   \end{achievements}

\end{resume}
