% !TeX root = ../thuthesis-example.tex

% 中英文摘要和关键字

\begin{abstract}
大语言模型通过大规模预训练和高质量数据微调获得了极高的通用性和专业性，成为当前迈向通用人工智能的最有潜力的路线。大语言模型的智能提升遵循扩展定律，这表明其性能随训练资源增加而提高。因此，人类资源的有限性成为制约大语言模型智能发展的核心瓶颈之一。本文致力于减少大语言模型训练过程中的资源消耗，从而促进模型规模的进一步发展。具体而言，本文研究了大语言模型训练过程中两个主要阶段——预训练阶段和微调阶段——各自面临的资源挑战。本文认为不可预测性导致预训练反复调优，进而浪费了大量资源。而高质量数据消耗量大和微调参数冗余度高导致了微调阶段的资源浪费。因此，本文提出了可预测预训练和少资源微调两个研究目标，并探讨了以下四个方面的内容：

\textbf{面向可预测预训练的超参数扩展规律与性能扩展定律。} 本文首先研究了超参数在模型规模扩展过程中的变化规律，使得其最优值在规模扩大过程中保持可预测，以便精准拟合损失扩展定律。随后，本文在损失扩展定律基础上提出了性能扩展定律，并且预测准确率达99\%以上，避免了大规模预训练的反复试错。

\textbf{面向可预测预训练的可复用扩展定律与高效退火迭代。} 本文接着研究了在扩展定律拟合过程中的资源开销，提出了WSD学习率调度，通过复用稳定训练阶段的检查点大幅节省实验量。基于该调度，本文进一步提出了大模型的高效迭代方案。最终，本文以低成本训练了一个性能优异的工业级语言模型MiniCPM。

\textbf{面向少资源微调的能力激发与知识激活。} 在微调方面，本文研究了如何利用预训练阶段的知识进行数据高效的下游任务适配。在以预训练数据分布外的错误前提问题为主题的研究中，本文发现只需要精标数百条数据即可让模型学会反驳错误前提问题。本文进一步通过引入知识库来激活预训练模型所具备的知识，让少样本提示微调的性能提高最多17\%，大幅降低所需样本量。

\textbf{面向少资源微调的参数高效微调模块自动化寻优。} 本文深入研究了参数高效微调方法面临的模块选择问题，提出了模块自动选择框架，实现了模块自动选择，使得参数高效微调的调优参数进一步缩小至万分之一量级，存算压力大幅减小。

总体而言，本文对大语言模型训练全流程提出了高效方案，有利于模型进一步进行规模扩展以实现更高的智能上限。与此同时，本文的研究成果对包括模型训练机理、模型参数功能分区、模型涌现能力成因等方面都提供了新的见解。


  % 关键词用“英文逗号”分隔，输出时会自动处理为正确的分隔符
  \thusetup{
    keywords = {大语言模型, 预训练, 扩展定律, 参数高效微调, 提示学习},
  }
\end{abstract}

\begin{abstract*}  
Large language models (LLMs) achieve remarkable generality and specialization through extensive pretraining and fine-tuning on high-quality data, making them one of the most promising routes toward artificial general intelligence. The development of LLMs follows the Scaling Law, which suggests that their performance improves with increased training resources. As such, the limited availability of human resources has become one of the core bottlenecks limiting the intelligent development of LLMs. This work aims to reduce the resource consumption during the training process of LLMs, thus promoting the further development of model scale. Specifically, this work addresses the resource challenges encountered in two main stages of LLMs: the pretraining phase and the fine-tuning phase. We argue that the unpredictability of pretraining leads to resource wastage due to iterative experiments, while the high consumption of high-quality data and redundancy in fine-tuning parameters result in waste during the fine-tuning process. Therefore, this thesis proposes two research objectives: predictable pretraining and low-resource fine-tuning, and investigates the following four key areas:

\textbf{Hyperparameter Scaling Laws and Performance Scaling Laws for Predictable Pretraining.} This thesis first explores the scaling patterns of optimal hyperparameters during model scaling, aiming to ensure their stability or predictability as the scale increases, thus facilitating the fitting of accurate loss scaling laws. Building upon these loss scaling laws, this thesis proposes a performance scaling law, with an accuracy of over 99\%, thus eliminating the need for trial-and-error in large-scale pretraining experiments.
  
\textbf{Reusable Scaling Laws and Efficient Annealing Iterations for Predictable Pretraining.} Next, this thesis investigates the resource overhead in fitting the scaling laws. We introduce the WSD learning rate scheduler, which significantly reduces experimental cost by reusing checkpoints from stable training phases. Furthermore, this thesis proposes an efficient iteration scheme for LLMs based on this scheduling. Finally, this thesis trains an industrial-level language model with excellent performance using limited resources.
  
\textbf{Capability and Knowledge Activation for Low-Resource Fine-Tuning.}
In terms of fine-tuning, this thesis examines how to leverage the knowledge from the pretraining phase for data-efficient adaptation to downstream tasks. In studies on addressing false premise questions beyond the pretraining data distribution, this thesis finds that only a few hundred labeled data points are necessary for the model to learn to refute false premise questions. Additionally, this thesis optimizes prompt-tuning by introducing a knowledge base to activate the knowledge embedded in the pretrained model, achieving up to a 17\% performance improvement in few-shot fine-tuning, significantly reducing the number of samples required.
  
\textbf{Automated Architecture Optimization for Parameter-efficient Tuning Methods.} This paper delves into the challenges of optimizing Delta-tuning methods, proposing an automatic module selection framework and enabling the automatic selection, which reduces the fine-tuning parameters of delta-tuning by an order of magnitude, alleviating the computational burden of maintaining large model replicas during the fine-tuning process.
  
In conclusion, this thesis presents efficient solutions for the entire training pipeline of LLMs, which are conducive to further scaling model sizes and the achievement of higher intelligence limits. Moreover, the findings contribute new insights into model training mechanisms, parameter functional partitioning, and the emergence of model capabilities.

  % Use comma as separator when inputting
  \thusetup{
    keywords* = {Large Language Models, Pretraining, Scaling Law, Delta-tuning, Prompt-tuning},
  }
\end{abstract*}
