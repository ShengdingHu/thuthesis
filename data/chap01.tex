% !TeX root = ../thuthesis-example.tex

\chapter{引言}

自从上世纪50年代人工智能概念提出以来，自然语言处理（NLP）经历了从基于规则到统计方法，再到机器学习的演变。早期的规则系统和统计模型虽然取得了一定进展，但受限于语言复杂性和数据规模，效果有限。直到2010年代，深度学习的兴起为NLP带来了突破，尤其是2018年Transformer模型的提出，彻底改变了这一领域。基于Transformer的大型预训练模型（如BERT、GPT系列）通过海量数据训练，在文本理解、生成和翻译等任务上表现出色，显著提升了机器处理自然语言的能力。


前沿的大模型范式主要包含两个阶段。第一个阶段是在大量无标注语料上进行大规模预训练，目标是构建一个能力足够通用且强大的基础模型（Foundation Model）。这一阶段的预训练使模型能够学习到语言的通用特征和知识，为后续任务打下坚实基础。第二阶段则是在下游任务上进行微调，即针对使用者特别关注的领域，对基础模型进行特定领域的继续训练，从而构建更精准的分支模型。这种两阶段范式显著提升了模型在多样化任务上的适应性和性能。

近年来，随着扩展定律（Scaling Law）的发现，构建越来越大的模型成为公司和机构共同追求的目标。更大的模型规模通常意味着更强的能力，尤其是在复杂任务上展现出惊人的潜力。然而，这些大模型的训练需要消耗巨大的计算资源和能源，训练过程不仅成本高昂，还可能对环境造成影响。如果训练方式不当或缺乏优化，将会造成极大的资源浪费。因此，如何在提升模型性能的同时实现资源的高效利用，已成为当前人工智能领域亟待解决的重要问题。本文在预训练-微调范式下，通过提高预训练可预测性、降低微调资源开销的总体思路，对训练开销巨大的大模型进行了全流程的训练效率改进。

本章将为读者介绍本文的研究背景，探讨预训练-微调范式下两阶段各自面临的训练效率挑战，并且详细阐明本文的研究内容和主要贡献。



\section{研究背景}

自然语言处理的发展历程本质上反映了人类对语言认知规律的探索深化与计算范式革新之间的双重变奏。1950年艾伦·图灵提出“模仿游戏”测试时，符号主义学派主导着早期NLP研究，诺姆·乔姆斯基的生成语法体系试图通过形式化规则解析语言结构。然而这种基于演绎逻辑的方法在面对自然语言的歧义性和语境依赖性时遭遇根本性困境，正如1966年ALPAC报告所指出的，单纯依靠人工编写语法规则难以实现真正的语言理解。

统计语言模型的兴起标志着NLP进入数据驱动的新阶段。1980年代隐马尔可夫模型在语音识别中的成功应用，1990年代最大熵模型和条件随机场在词性标注等序列标注任务中的突破，逐渐确立了概率图模型的主导地位。这一时期的里程碑式进展背后，是香农信息论与贝叶斯统计的深度融合，语言处理开始被视为概率空间中的最优推断问题。然而，传统统计方法严重依赖特征工程，其浅层模型架构难以捕捉语言的多层次抽象特性。

深度学习的革命性突破始于2013年词向量(word2vec)技术的出现，通过分布式表示实现了语义空间的连续映射。长短时记忆网络(LSTM)和门控循环单元(GRU)在机器翻译等任务中展现出的序列建模能力，使得神经网络开始全面取代统计模型。但真正引发范式转换的是2017年Transformer架构的提出——其自注意力机制突破了RNN的序列计算瓶颈，使并行化训练超长文本序列成为可能。2018年BERT模型首次验证了预训练-微调范式的有效性，通过掩码语言建模(MLM)任务使模型学习到深层次的语境化表示，由此开启了大规模预训练语言模型的时代。

这一“预训练-微调”技术演进背后蕴含着深刻的神经科学启示。大脑皮层的层次化信息处理机制为深度神经网络提供了生物学基础，赫布理论(Hebbian Theory)关于"神经元同步激活则连接增强"的学说，与自监督预训练中的权重更新机制形成跨学科呼应。特别是预测编码理论(Predictive Coding)指出，大脑通过不断预测感官输入来构建世界模型，这与语言模型基于上文预测下文的自回归机制具有惊人的相似性。这些认知科学的理论积淀，为当前预训练范式提供了超越工程实践层面的理论支撑。

当前大模型训练已进入百万亿参数时代，其资源消耗呈指数级增长趋势。OpenAI的GPT-3模型训练需要消耗$3.14\times 10^{23}$ FLOPs运算量，仅单次训练产生的碳排放就达500吨以上。更严峻的是，由于扩展定律(Scaling Law)揭示的性能与模型规模、数据量、计算量的幂律关系，追求更大规模模型成为必然选择。

在微调阶段，虽然所需计算量通常比预训练低很多个数量级，但面临更复杂的优化曲面。一方面，下游任务的多样性要求为每个任务保存独立的全参数模型副本，另一方面， 预训练模型蕴含的通用语言知识与特定任务需求之间存在复杂映射关系，仅仅基于梯度下降的微调过程不能拿有效利用预训练获得的先验能力，从而在样本利用效率上大打折扣。更实际的问题是，微调阶段资源的提供方通常只有非常少的算力，无法进行大数据，全参数的微调。



\section{研究意义}
大模型训练是大模型开发生命周期的关键环节，预训练质量决定大模型能力上限，微调效果影响其应用适配度。为提升大模型训练两阶段的效率，我们针对不同痛点，研究可预测预训练技术和低资源微调技术。以下从四个角度探讨本文高效训练方法的研究价值：

\begin{enumerate}
  \item 降低预训练成本与提升成功率。深度学习网络性能受网络架构、训练超参数等多种因素影响，传统研究常需大量调参实验确定最佳训练配置。进入预训练时代，大模型训练成本飙升。当前领先的预训练模型规模达千亿参数量、十万亿数据量（按词元统计），计算成本极高，且实验耗时久，迭代效率低。可预测预训练将目标网络性能转化为函数外插拟合问题，通过小规模模型快速迭代实验，经精细扩展定律拟合外推至大模型，变同规模大量参数搜索为小规模快速迭代，大幅降低实验成本。同时，大规模深度学习网络训练常遇不稳定、效果不佳问题，如损失突增、爆炸等。可预测预训练能提前预判此类问题，实现早干预、早调整，提升预训练成功率。
  \item 减少下游任务适配的数据与硬件需求。下游任务适配和预训练面临不同效率问题。下游任务需求由用户定义，传统微调虽无需预训练量级数据，但需专有领域万量级数据。本文研究的提示学习大幅减少数据需求，使预训练模型经零样本或少样本训练就能在下游任务取得良好性能。此外，下游任务使用者通常缺乏大模型预训练机构的大量计算和存储资源，即便依靠预训练机构微调，机构也难提供参数各异的任务特异模型。本文研究使不同任务资源需求降至普通全参数微调的万分之一，大幅降低计算资源。
  \item 增强对未来技术的预判能力。可预测预训练技术不仅能提高当前大模型训练效率，还能增强对未来技术的预判。大模型预训练核心是找到可扩展技术路线，不同路线扩展斜率不同。该技术让研究者更好预判特定技术路线下模型发展潜力，提前知晓技术上限和潜力。了解当前技术上限可催生新技术，知晓不同技术潜力能合理分配资源，有助于对未来技术发展做出更好决策。
  \item 促进大模型社区的多样性与繁荣。降低下游任务适配的数据和硬件需求，将使更多任务能以低门槛适配大模型，增加大模型社区多样性。即便使用同一基础模型，也会因适配数据和参数高效微调模块不同，展现差异化能力。同时，门槛降低会吸引更多行业成员活跃于大模型社区，创造更多分支模型，提升社区繁荣度。
\end{enumerate}


\section{研究内容与主要贡献}
本文设计了一套全流程的高效训练方案，如图所示。高效性主要体现在两个阶段：（1）预训练阶段，通过引入超参可预测扩展，扩展定律拟合，下游任务性能预测来完成固定初步扩展定律的构建。并通过新型学习率调度，新型数据策略，来完成扩展定律的低资源测量，并以此构建更好的模型。 （2）微调阶段，通过引入提示学习和能力激活，完成微调的数据量优化，通过引入自动调优的参数高效微调方法，完成微调的参数量优化。

具体而言，

\begin{figure}
\centering
\includegraphics[width=\textwidth]{chap01/phdthesismain.pdf}
\label{fig:mainframework}
\caption{本文主要研究内容。}
\end{figure}

\section{本文组织结构}


1. 大模型能力观察
Won't get fooled again: Answering questions with false premises

2.1 高效方法之一 -- 科学化预测
Predicting Emergent Abilities with Infinite Resolution Evaluation

2.2 高效方法之二 -- 可扩展训练策略
Minicpm: Unveiling the potential of small language models with scalable training strategies
A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules (部分)

2.3 高效方法之数据 -- 精选数据让模型
DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models


3.1 大模型能力高效利用
Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification
Sparse structure search for delta tuning

4.1 产出的工具包
OpenDelta: A plug-and-play library for parameter-efficient adaptation of pre-trained models


\section{论文题目的写法}

论文题目应简明扼要地反映论文工作的主要内容，力求精炼、准确，切忌笼统。
论文题目是对研究对象的准确、具体描述，一般要在一定程度上体现研究结论，因此，论文题目不仅应告诉读者这本论文研究了什么问题，更要告诉读者这个研究得出的结论。
例如：“在事实与虚构之间：梅乐、卡彭特、沃尔夫的新闻观”就比“三个美国作家的新闻观研究”更专业、更准确。



\section{摘要的写法}

论文摘要是对论文研究内容的高度概括，应具有独立性和自含性，即应是 一篇简短但意义完整的文章。
通过阅读论文摘要，读者应该能够对论文的研究 方法及结论有一个整体性的了解，因此摘要的写法应力求精确简明。
论文摘要 应包括对问题及研究目的的描述、对使用的方法和研究过程进行的简要介绍、 对研究结论的高度凝练等，重点是结果和结论。

论文摘要切忌写成全文的提纲，尤其要避免“第 1 章……；第 2 章……；……”这样的陈述方式。



\section{引言的写法}

一篇学位论文的引言大致包含如下几个部分：
1、问题的提出；
2、选题背 景及意义；
3、文献综述；
4、研究方法；
5、论文结构安排。
\begin{itemize}
  \item 问题的提出：要清晰地阐述所要研究的问题“是什么”。
    \footnote{选题时切记要有“问题意识”，不要选不是问题的问题来研究。}
  \item 选题背景及意义：论述清楚为什么选择这个题目来研究，即阐述该研究对学科发展的贡献、对国计民生的理论与现实意义等。
  \item 文献综述：对本研究主题范围内的文献进行详尽的综合述评，“述”的同时一定要有“评”，指出现有研究状态，仍存在哪些尚待解决的问题，讲出自己的研究有哪些探索性内容。
  \item 研究方法：讲清论文所使用的学术研究方法。
  \item 论文结构安排：介绍本论文的写作结构安排。
\end{itemize}

