% !TeX root = ../thuthesis-example.tex

\chapter{可预测预训练——超参数扩展规律与性能扩展定律}

预训练阶段由于模型和数据量都十分庞大，进行一次充分训练的成本高昂。本章开始介绍为了提高预训练效率而发展出的可预测预训练技术。预训练模型的训练过程受超参数和评估体系的显著影响。因此，本章从超参数和性能评估出发，研究如何建立可预测的预训练模型训练规律。

\section{超参数扩展规律}
作为本章的第一部分，本节将深入研究训练超参数对训练的作用，尝试对其进行预测，以期在大规模语言模型上不需要进行超参数搜索即可使用最优超参。

\subsection{背景}

在深度学习模型的训练中，超参数起着关键作用，直接影响模型的训练效率、性能表现以及最终的泛化能力。众所周知，深度学习的超参数包括架构参数、参数初始化、学习率、学习率调度方案、批量大小、批量大小调度方案、权重衰减（weight decay）、训练词元数等诸多影响因素。
其中部分参数对模型规模的变化不敏感。例如架构参数是扩展的先决条件，通常以恒定模型规模比扩展，而权重衰减通常设置为0.1左右。因此本文主要研究包括参数初始化、学习率、学习率调度方案、批量大小等随模型规模扩大变化较大的参数。

\paragraph{参数初始化方案}
参数初始化是模型训练的起点，合理的初始化方案能够加速模型收敛，避免梯度消失或梯度爆炸等问题。常见的初始化方法包括随机初始化和基于特定分布的初始化。随机初始化通常是从均匀分布或正态分布中随机采样数值来初始化模型参数。例如，在神经网络中，权重矩阵可以通过均匀分布初始化，使每个参数在一定范围内取值，这有助于打破模型训练的对称性，防止所有神经元在训练过程中表现一致。
另一种常用的初始化方法是基于特定分布的初始化，如Xavier初始化~\cite{pmlr-v9-glorot10a}和He初始化~\cite{he2015delving}。Xavier初始化根据输入和输出神经元的数量来调整初始化参数的标准差，使得在正向传播和反向传播过程中，信号能够在各层之间较为均匀地传递，从而提高训练的稳定性。He初始化则针对ReLU激活函数进行了优化，它能够更好地适应 ReLU 函数在负半轴输出为0的特性，使得模型在使用 ReLU 激活函数时能够更快地收敛。
本章对参数初始化方案进行了调整，主要参考来自Tensor Program~\cite{yang2022tensor}的初始化方案，结合实际大语言模型训练的实际情况，实现稳定的规模化。


\paragraph{批量大小（Batch Size）}
批量大小指的是在一次训练迭代中，用于计算梯度和更新参数的样本数量。较大的批量大小可以利用更多的数据信息来计算梯度，使得梯度估计更加准确，从而加速模型的收敛。然而，较大的批量大小也会带来更高的内存需求和计算成本，并且可能导致模型在训练过程中过拟合。
较小的批量大小可以在每次迭代中更快地更新参数，增加模型训练的随机性，有助于跳出局部最优解。但同时，由于每次使用的数据量较少，梯度估计的噪声较大，可能会导致模型收敛不稳定。在传统深度学习的训练中，需要根据数据集的大小、模型的复杂度以及硬件资源等因素来选择合适的批量大小。通常，可以通过实验来探索不同批量大小对模型性能的影响，从而找到最优的设置。然而在大语言模型训练中，无法在每次大规模实验前仔细调整批量大小，因此研究批量大小如何随着模型规模扩大变得很重要。

\paragraph{学习率}
学习率决定了模型在训练过程中参数更新的步长。如果学习率设置过小，模型的训练过程会非常缓慢，需要更多的训练轮次才能达到较好的收敛效果；而如果学习率设置过大，模型可能会在训练过程中无法收敛，甚至出现损失函数不断增大的情况。
常见的学习率调整策略包括固定学习率、学习率衰减和自适应学习率。固定学习率在整个训练过程中保持不变，适用于一些简单的模型和数据集。学习率衰减则是在训练过程中逐渐降低学习率，这样可以在训练初期快速更新参数，加速收敛，而在训练后期减小步长，避免错过最优解。常见的学习率衰减方法有指数衰减、余弦退火衰减等。在模型规模扩大过程中，最优的学习率通常不是一成不变的。因此，如果不预测或稳定最优学习率，无法保证训练结果是最优的。本文将结合多方调整，使最优学习率保持稳定。

\subsection {方法设计}
\subsubsection {参数初始化设置}
张量程序（Tensor Program）\citep{yang2022tensor, yang2023tensor} 提出了一个理论框架，用于稳定不同规模模型的超参数。张量程序的主要部分是宽度缩放\citep{yang2022tensor} 和深度缩放\citep{yang2023tensor}。CerebrasGPT~\citep{dey2023cerebras} 的训练中就使用了宽度缩放的张量程序。本文从实验上对这些技术进行了广泛尝试，有选择地使用了其中几种作为最终方案。

张量程序的操作包括：
\begin{enumerate}
  \item 嵌入层输出缩放：将嵌入层的输出乘以数值$\text{scale\_\text{emb}}$。
  \item 残差连接缩放：在将每个层的块的输出张量添加到每个残差连接之前，将其按$\text{scale\_\text{depth}}/\sqrt{\text{num\_layers}}$进行缩放。
  \item 张量初始化：将每个二维张量参数的初始化标准差设置为$\text{init\_std}/\sqrt{d_m/d_{base}}$，并将其他参数的初始化设置为0.1。其中$d_m$是张量的维度，$d_{base}$是一个预先选定的基础维度。
  \item 张量的学习率缩放：将每个二维张量参数的学习率调整为其他部分（或整体学习率）学习率的$1/({d_m/d_{base}})$倍。
  \item 语言模型输出层缩放：将输出的对数几率调整为原始值的$1/(d_m/d_{base})$倍。
\end{enumerate}

张量程序中还提出了对注意力头归一化因子的缩放，而由于在大语言模型的实际应用中，多注意力头的维度一般为64维或者128维，而只会通过增加多注意力头的个数来进行参数扩展。出于这点考虑，本文没有应用\citet{yang2022tensor} 中提出的注意力头归一化因子的缩放技术。本研究结果与\citet{yang2023tensor}中不同之处在于，张量程序提出对于深度大于2的网络，深度缩放效果不佳，但本文通过实验发现，结合下文的调整，最优学习率是稳定的。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{chap03/mup0.009bwith10Ndata.pdf}
  \caption{使用张量程序后的贝叶斯超参搜索}
  \label{fig:mupsearch_app}
\end{figure}

\subsubsection{贝叶斯超参数搜索}
基于张量程序的超参数扩展方案虽然解决了一些超参数的配置问题，但是却引入了另外三个模型大小无关超参数。为了确定这三个超参数，本研究在一组预定义的参数空间上进行了广泛的贝叶斯搜索。本研究针对 $N = 0.009B$ 的模型进行搜索。为了进一步减少实验所需的资源，在前期实验中，本文证实，当使用规模为 $10N$ 和 $20N$ 数量级的数据集进行超参数优化时，超参数的有效性呈现出一致性。因此，本文假设这些超参数的选择不会随数据规模扩大而变化，于是使用 $|D| = 10N = 0.09B$ 个词元（tokens）来训练模型。

同时，本文还尝试了QK-Norm~\citep{henry-etal-2020-query}和独立权重衰减~\citep{loshchilov2017decoupled}以稳定学习率。QK-Norm在Transformer的自注意力机制的查询、键向量后加入了一个归一化层，以稳定在层数变深和维度变宽过程中的输入输出的数值范围变化。独立权重衰减则是将权重衰减（weight decay）和学习率调度独立，使用整个训练过程统一的权重衰减系数。包括这两项的整体超参搜索结果如图\ref{fig:mupsearch_app}所示。应用QK-Norm后，本文观察到学习率敏感性显著降低，这与~\citet{wortsman2023small}的情况类似。根据图\ref{fig:mupsearch_app}，本文确定了最佳超参数为 $\text{scale\_depth} = 1.4$，$\text{scale\_emb} = 12$，$\text{init\_std = 0.1}$ 以及学习率$\text{learning\_rate} = 0.01$。



\subsubsection{最优批量大小}

批量大小决定了模型收敛速度与计算资源消耗之间的平衡。如果批量大小过大，将导致大量的数据和计算资源消耗。另一方面，如果批量大小过小，则需要大量的训练步骤，并且可能导致损失函数的下降有限。本研究参考\citet{kaplan2020scaling} 中从预期损失确定批量大小的方法，设计了对于本研究所针对的有限计算资源的修改方法。


在~\citet{kaplan2020scaling}中，OpenAI研究了损失函数与词元数量之间的关系。在他们的实验中，他们认为消耗更多的训练步骤等同于消耗更多的时间资源。在这一观点下，OpenAI定义了一个关键批量大小，该批量大小可以在不消耗过多训练步骤或词元的情况下实现一定的损失。如果实验提供了无限的GPU资源，这一理由是有效的。由于GPU是无限的，增大批量大小不会增加单步时间，但会减少总步骤数。然而，在本文的实验中，由于本文拥有固定的资源（GPU数量），本文观察到在将GPU计算打满的情况下（即计算瓶颈，而非访存和卡间通信瓶颈），批量大小翻倍几乎等同于将单步时间翻倍。因此，通过增大批量大小来减少总训练步骤对总训练时间的影响微乎其微。基于这一观察，本文放弃了“不消耗过多步骤”的目标，转而专注于最小化词元数量以实现最低损失。


为实现这一目标，本文分别在 0.09 亿、0.3 亿和 1.7 亿参数规模的模型上进行实验。每个模型规模在全局学习率为 0.01 且使用余弦退火学习率调度器的情况下，在 6 种批量大小下进行训练。本文观察了在 C4~\citep{2019t5} 数据集上最优批量大小随损失的变化趋势（图 \ref{fig:optimalbatchsize} 中的红线）。

\begin{figure}[!htbp]
\centering
\includegraphics [width=\linewidth]{chap03/batch_size_1.png}
\caption {三种规模模型在不同批量大小下训练的损失曲线}
\label {fig:optimalbatchsize}
\end{figure}


\begin{figure}[!htbp]
\centering
\includegraphics [width=0.55\linewidth]{chap03/batch_size_2.zh.pdf}
\caption {相连的最优批量大小}
\label {fig:optimalbatchsizeconnect}
\end{figure}


如图 \ref{fig:optimalbatchsize} 所示，本文将批量大小绘制在 x 轴上，词元消耗绘制在 y 轴上，点的颜色代表损失。具有渐变颜色的点形成的每条垂直线代表一条训练曲线。颜色越浅表示损失越高。本文使用抛物线拟合等损失点，并用红线连接抛物线的最小值。这些线表明随着损失降低，最优批量大小向更大值移动。然后连接这三条线（见图 \ref{fig:optimalbatchsizeconnect}），发现在对数空间中这些线很好地连接成一种线性关系，由此本文得到批量大小（$\text{batch\_size}$）与 C4损失（L）之间的以下关系：$ \text{batch\_size} = \frac{1.21\times10^9}{L^{6.24}}$。

可以看到，上述式子的最优批量大小是损失的函数，这可能带来一定的困惑：到底是先确定损失还是先确定最优批量大小？关于最优批量大小与损失之间关系的测量类似于“鸡与蛋”悖论。实际上，本文通过在固定批量大小下先进行损失缩放研究，得到损失初步估计后，再代入批量大小估计式得到更优的批量大小。 这有点类似于坐标下降优化方法。然而，未来有可能开发出更精细的估计方法。


\subsection{实验}

\subsubsection{实验配置}
本文在表~\ref{tab:appmodel_configs}中列出了在模型规模扩增实验中使用的模型配置。N（B）表示模型的非嵌入参数数量，单位为十亿（B）。力图让模型的“形状”，即模型宽度与模型深度的比例，尽可能保持一致，以避免任何潜在的性能变化。其中$d_m$是模型隐状态维度，$d_{ff}$ 是前馈网络中间层维度，$d_h$是多自注意力各头的维度，$n_h$是多自注意力头的个数，$L$是模型层数。

\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.42\linewidth}
      \vspace{-6mm}
      \centering
      \subcaption{缩放曲线中模型的配置}
      \scalebox{0.8}{
      \begin{tabular}{c|cccccc}
      \toprule
          \textbf{名称} & \textbf{N （B）}& $d_m$ & $d_{ff}$ &$d_h$ & $n_h$ & $L$ \\
      \midrule
            0.09亿   &  0.009 & 320 & 800 & 64 & 5 & 8\\
             0.4亿 &   0.036 & 512 & 1280 & 64 & 8 & 12  \\
            0.7亿 &  0.066 & 640 & 1600 & 64 & 10 & 14\\
            1亿 &  0.109 & 768 & 1920 & 64 & 12 & 16  \\
           1.7亿 &  0.166 & 896 & 2240 & 64 & 14 & 18 \\
           2亿&  0.241 & 1024 & 2560 & 64 & 16 & 20 \\
          5亿& 0.499 & 1344 & 3360 & 64 & 21 & 24 \\
      \bottomrule
      \end{tabular}
      }
      \label{tab:appmodel_configs}
  \end{minipage}
  \hfill
  \begin{minipage}{0.52\linewidth}
    \vspace{5mm}
      \centering
      \subcaption{损失关于学习率的变化图像}
      \includegraphics[width=\linewidth]{chap03/loss_vs_lr.zh.pdf}
      \label{fig:loss_vs_lr}
  \end{minipage}
  \caption{模型配置与学习率影响}
\end{figure}


由于本文使用了张量程序的理论~\citep{yang2022tensor, yang2023tensor}，且进行了批量大小缩放，本文预计在模型缩放过程中学习率不会发生显著变化。为了验证这一点，本文在 0.4 亿、1 亿、3 亿和 5 亿参数规模上进行了六组学习率实验。在图 \ref {fig:loss_vs_lr} 中，本文发现尽管模型规模增加了十余倍，但最优基础学习率 \footnote{二维张量的实际学习率将在基础学习率的基础上，根据张量程序的理论进行缩放。} 并没有明显变化，仍保持在 0.01 左右。本文进一步在 21 亿参数规模上进行了简单验证，证实学习率为 0.01 确实能达到最低损失。

\subsubsection{损失扩展定律验证}
图\ref{fig:loss_scaling}展示了各模型的训练损失曲线，内插图展示了相对于模型规模的最终步骤可约损失，以对数尺度表示。显然，最终训练损失按照扩展定律下降。这些通过实证观察得到的损失扩展定律，为后续对任务性能的分析奠定了基础。需要注意的是，尽管15亿和24亿参数的模型出现了损失尖峰，但最终仍收敛到扩展定律，这体现了该经验定律的稳健性。


\begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.96\linewidth]{pufigs/train_loss_2series.zh.pdf}
        \label{fig:train_loss_2series}
\caption{在不同数据混合上训练的两个系列模型的训练损失}
    \label{fig:loss_scaling}
\end{figure}

  % Second minipage for the second figure
%   \begin{minipage}{0.46\linewidth}
%     \centering
%     \includegraphics[width=1.0\linewidth]{Fig/cosine_2024-03-26_15-36-16.pdf}
%     \caption{Cosine Learning Rate Scheduler with different periods. The Y-axis is the loss on the C4 corpus.}
%     \label{fig:cosine_lr}
%     \vspace{0.47cm}
% \end{minipage}
% \end{figure}
% \vspace{-5mm}

